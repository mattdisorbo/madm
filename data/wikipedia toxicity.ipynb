{
  "cells": [
    {
      "cell_type": "code",
      "id": "b4J6KIqaL0FJdDsupycOW4QA",
      "metadata": {
        "tags": [],
        "id": "b4J6KIqaL0FJdDsupycOW4QA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769697767476,
          "user_tz": 300,
          "elapsed": 15303,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from pydantic import BaseModel, Field\n",
        "import matplotlib.pyplot as plt\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "# from anthropic import Anthropic\n",
        "\n",
        "# key from delegation OpenAI project\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "\n",
        "# MODEL = \"gpt-4o-2024-05-13\"\n",
        "# MODEL = \"gpt-4o-mini-2024-07-18\"\n",
        "MODEL = \"gpt-5-nano\" # cheaper than 4o-mini!\n",
        "# MODEL = \"o1-2024-12-17\"\n",
        "# MODEL = \"o3-mini-2025-01-31\"\n",
        "# MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
        "# MODEL = \"gemini-2.5-flash\"\n",
        "\n",
        "# list of claude models here: https://docs.anthropic.com/en/docs/about-claude/models/overview\n",
        "# MODEL = \"claude-3-5-haiku@20241022\"\n",
        "# MODEL = \"claude-sonnet-4@20250514\"\n",
        "# MODEL = \"claude-opus-4-20250514\"\n",
        "\n",
        "# list of llama models here: https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-scout?utm_source=chatgpt.com\n",
        "# MODEL = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
        "# MODEL = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
        "\n",
        "# initialize client\n",
        "client = genai.Client(\n",
        "    vertexai=True,\n",
        "    # project=\"accuracy-obsession\",  # this links to my CC! Although credits might be in there before. Exceptions project is exceptions-467800\n",
        "    # project = \"exceptions-467800\",\n",
        "    location=\"us-central1\"\n",
        ")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "# https://figshare.com/articles/dataset/Wikipedia_Talk_Labels_Toxicity/4563973\n",
        "\n",
        "# load\n",
        "# toxicity_comments = pd.read_csv('gs://exceptions-data/LLM Delegation/Wikipedia Toxicity/data/toxicity_annotated_comments.tsv', sep = '\\t')\n",
        "# toxicity = pd.read_csv('gs://exceptions-data/LLM Delegation/Wikipedia Toxicity/data/toxicity_annotations.tsv', sep = '\\t')\n",
        "# toxicity_demographics = pd.read_csv('gs://exceptions-data/LLM Delegation/Wikipedia Toxicity/data/toxicity_worker_demographics.tsv', sep = '\\t')\n",
        "\n",
        "# merge into dataset\n",
        "# data = pd.merge(toxicity_comments, toxicity, on = 'rev_id')\n",
        "# data = pd.merge(data, toxicity_demographics, on = 'worker_id', how = 'left')\n",
        "\n",
        "# save back, just load this\n",
        "# data.to_csv(\"gs://exceptions-data/LLM Delegation/Wikipedia Toxicity/data/data.csv\", index = False)\n",
        "\n",
        "# load merged data\n",
        "data = pd.read_csv(\"gs://exceptions-data/LLM Delegation/Wikipedia Toxicity/data/data.csv\")\n",
        "\n",
        "# rev_id: ID of the revision (I think the comment)\n",
        "# worker_id: ID of the worker who gave the score\n",
        "# toxicity_score: -2 (very toxic) to 2 (very healthy)\n",
        "# toxic: 1 if toxicity_score < 0, else 0\n",
        "\n",
        "# clean up comments\n",
        "data[\"comment\"] = (\n",
        "    data[\"comment\"]\n",
        "        .str.replace(\"NEWLINE_TOKEN\", \" \\n \", regex=False)\n",
        "        .str.replace(\"TAB_TOKEN\", \" \\t \", regex=False)\n",
        ")\n"
      ],
      "metadata": {
        "id": "s8x-ExqYdXho",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769699619141,
          "user_tz": 300,
          "elapsed": 23949,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "s8x-ExqYdXho",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "en3-PmUeS7HC",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769699619141,
          "user_tz": 300,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d26f37c2-aaf7-48b4-a745-9019e8590816"
      },
      "id": "en3-PmUeS7HC",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   rev_id                                            comment  year  logged_in  \\\n",
              "0  2232.0  This: \\n :One can make an analogy in mathemati...  2002       True   \n",
              "1  2232.0  This: \\n :One can make an analogy in mathemati...  2002       True   \n",
              "2  2232.0  This: \\n :One can make an analogy in mathemati...  2002       True   \n",
              "3  2232.0  This: \\n :One can make an analogy in mathemati...  2002       True   \n",
              "4  2232.0  This: \\n :One can make an analogy in mathemati...  2002       True   \n",
              "\n",
              "        ns  sample  split  worker_id  toxicity  toxicity_score  gender  \\\n",
              "0  article  random  train        723         0             0.0  female   \n",
              "1  article  random  train       4000         0             0.0    male   \n",
              "2  article  random  train       3989         0             1.0  female   \n",
              "3  article  random  train       3341         0             0.0     NaN   \n",
              "4  article  random  train       1574         0             1.0  female   \n",
              "\n",
              "   english_first_language age_group  education  \n",
              "0                     0.0     30-45  bachelors  \n",
              "1                     1.0     18-30         hs  \n",
              "2                     0.0     18-30  bachelors  \n",
              "3                     NaN       NaN        NaN  \n",
              "4                     0.0     30-45  bachelors  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9b10e30-035d-4118-a7bd-2966a5cafcba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rev_id</th>\n",
              "      <th>comment</th>\n",
              "      <th>year</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>ns</th>\n",
              "      <th>sample</th>\n",
              "      <th>split</th>\n",
              "      <th>worker_id</th>\n",
              "      <th>toxicity</th>\n",
              "      <th>toxicity_score</th>\n",
              "      <th>gender</th>\n",
              "      <th>english_first_language</th>\n",
              "      <th>age_group</th>\n",
              "      <th>education</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2232.0</td>\n",
              "      <td>This: \\n :One can make an analogy in mathemati...</td>\n",
              "      <td>2002</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>723</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30-45</td>\n",
              "      <td>bachelors</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2232.0</td>\n",
              "      <td>This: \\n :One can make an analogy in mathemati...</td>\n",
              "      <td>2002</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>4000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>male</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18-30</td>\n",
              "      <td>hs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2232.0</td>\n",
              "      <td>This: \\n :One can make an analogy in mathemati...</td>\n",
              "      <td>2002</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>3989</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18-30</td>\n",
              "      <td>bachelors</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2232.0</td>\n",
              "      <td>This: \\n :One can make an analogy in mathemati...</td>\n",
              "      <td>2002</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>3341</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2232.0</td>\n",
              "      <td>This: \\n :One can make an analogy in mathemati...</td>\n",
              "      <td>2002</td>\n",
              "      <td>True</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>1574</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>female</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30-45</td>\n",
              "      <td>bachelors</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9b10e30-035d-4118-a7bd-2966a5cafcba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a9b10e30-035d-4118-a7bd-2966a5cafcba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a9b10e30-035d-4118-a7bd-2966a5cafcba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Aggregate by rev_id. Unlike MoralMachine, there are 8+ #\n",
        "# workers for each comment (and usually exactly 10)      #\n",
        "##########################################################\n",
        "\n",
        "# columns to save\n",
        "meta_cols = [\"year\", \"ns\", \"sample\", \"split\", \"comment\"]\n",
        "\n",
        "# check that metadata is constant within each rev_id\n",
        "# if this fails, decide how to resolve conflicts (first/mode/etc.)\n",
        "bad = (\n",
        "    data.groupby(\"rev_id\")[meta_cols]\n",
        "        .nunique()\n",
        "        .gt(1)\n",
        "        .any(axis=1)\n",
        ")\n",
        "if bad.any():\n",
        "    raise ValueError(f\"Metadata columns vary within rev_id for {bad.sum()} rev_id(s). Example: {bad[bad].index[:5].tolist()}\")\n",
        "\n",
        "# aggregate\n",
        "data_agg = (\n",
        "      data.groupby(\"rev_id\", as_index=False)\n",
        "        .agg(\n",
        "            toxicity_score_mean=(\"toxicity_score\", \"mean\"),\n",
        "            comment=(\"comment\", \"first\"),\n",
        "            year=(\"year\", \"first\"),\n",
        "            ns=(\"ns\", \"first\"),\n",
        "            sample=(\"sample\", \"first\"),\n",
        "            split=(\"split\", \"first\"),\n",
        "        )\n",
        ")\n",
        "\n",
        "# define toxicity label\n",
        "data_agg[\"toxicity\"] = np.where(data_agg[\"toxicity_score_mean\"] < 0, 1, 0)\n"
      ],
      "metadata": {
        "id": "LBIBYxhjSWK_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769699621795,
          "user_tz": 300,
          "elapsed": 2657,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "LBIBYxhjSWK_",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_agg.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KTykwoc0R6M6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769699622379,
          "user_tz": 300,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "dc836713-f902-4cb7-e431-cfc9c223190c"
      },
      "id": "KTykwoc0R6M6",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    rev_id  toxicity_score_mean  \\\n",
              "0   2232.0                  0.4   \n",
              "1   4216.0                  0.5   \n",
              "2   8953.0                  0.1   \n",
              "3  26547.0                  0.6   \n",
              "4  28959.0                  0.2   \n",
              "\n",
              "                                             comment  year       ns  sample  \\\n",
              "0  This: \\n :One can make an analogy in mathemati...  2002  article  random   \n",
              "1  ` \\n  \\n :Clarification for you  (and Zundark'...  2002     user  random   \n",
              "2                          Elected or Electoral? JHK  2002  article  random   \n",
              "3  `This is such a fun entry.   Devotchka \\n  \\n ...  2002  article  random   \n",
              "4  Please relate the ozone hole to increases in c...  2002  article  random   \n",
              "\n",
              "   split  toxicity  \n",
              "0  train         0  \n",
              "1  train         0  \n",
              "2   test         0  \n",
              "3  train         0  \n",
              "4   test         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-357e4893-aaf5-4a08-be39-9b1a2cc3305c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rev_id</th>\n",
              "      <th>toxicity_score_mean</th>\n",
              "      <th>comment</th>\n",
              "      <th>year</th>\n",
              "      <th>ns</th>\n",
              "      <th>sample</th>\n",
              "      <th>split</th>\n",
              "      <th>toxicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2232.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>This: \\n :One can make an analogy in mathemati...</td>\n",
              "      <td>2002</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4216.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>` \\n  \\n :Clarification for you  (and Zundark'...</td>\n",
              "      <td>2002</td>\n",
              "      <td>user</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8953.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>Elected or Electoral? JHK</td>\n",
              "      <td>2002</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>test</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26547.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>`This is such a fun entry.   Devotchka \\n  \\n ...</td>\n",
              "      <td>2002</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28959.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Please relate the ozone hole to increases in c...</td>\n",
              "      <td>2002</td>\n",
              "      <td>article</td>\n",
              "      <td>random</td>\n",
              "      <td>test</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-357e4893-aaf5-4a08-be39-9b1a2cc3305c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-357e4893-aaf5-4a08-be39-9b1a2cc3305c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-357e4893-aaf5-4a08-be39-9b1a2cc3305c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_agg"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# scenario builder\n",
        "def create_prompt_base(row):\n",
        "    return f\"You are helping a group of crowd-workers label Wikipedia discussion comments as toxic or not. This comment needs to be checked: '{row['comment']}'.\"\n"
      ],
      "metadata": {
        "id": "_xcG5mOWUbxM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769699805783,
          "user_tz": 300,
          "elapsed": 155,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "_xcG5mOWUbxM",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_prompt_base(data_agg.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "HF6jo56JU7ro",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769699793805,
          "user_tz": 300,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "bea2fffe-0aa1-4963-c0b8-c168dc83b1b0"
      },
      "id": "HF6jo56JU7ro",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You are helping label Wikipedia discussion comments as toxic or not. This comment needs to be checked: This: \\n :One can make an analogy in mathematical terms by envisioning the distribution of opinions in a population as a Gaussian curve. We would then say that the consensus would be a statement that represents the range of opinions within perhaps three standard deviations of the mean opinion.  \\n sounds arbitrary and ad hoc.  Does it really belong in n encyclopedia article?  I don't see that it adds anything useful. \\n  \\n The paragraph that follows seems much more useful.  Are there any political theorists out there who can clarify the issues?  It seems to me that this is an issue that Locke, Rousseau, de Toqueville, and others must have debated...  SR \\n .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "\n",
        "# base LLM prompt\n",
        "def get_llm_base(base_prompt, model_id=MODEL):\n",
        "\n",
        "    # 1. Construct the one-shot prompt\n",
        "    instructions = (\n",
        "        \"\\n\\nRespond with two numbers ONLY on two lines. \"\n",
        "        \"First, respond 1 (toxic) or 0 (not toxic) based on your prediction of the crowd-workers' true aggregate decision. \\n\"\n",
        "        \"Second, respond 0 to implement your prediction of the crowd-workers' true aggregate decision, \"\n",
        "        \"or 1 if you would like to delegate to the crowd-workers so they can implement their true aggregate decision. \\n\"\n",
        "        \"A decision is considered 'correct' if it is equal to the crowd-workers' true aggregate decision.\"\n",
        "    )\n",
        "\n",
        "    full_prompt = base_prompt + instructions\n",
        "\n",
        "    # 2. Call the LLM\n",
        "\n",
        "    # openAI version\n",
        "    messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
        "    response = openai.chat.completions.create(model=model_id, messages=messages)\n",
        "    lines = response.choices[0].message.content.strip().split('\\n')\n",
        "\n",
        "    # gemini version\n",
        "    # response = client.models.generate_content(\n",
        "    #    model=model_id,\n",
        "    #    contents=full_prompt\n",
        "    # )\n",
        "    # lines = response.text.strip().split('\\n')\n",
        "\n",
        "\n",
        "    # 3. Parse the response (extract numbers from the two lines)\n",
        "    try:\n",
        "\n",
        "\n",
        "        # Filter out empty lines if the LLM added extra spacing\n",
        "        clean_lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "        # save integer if there is one\n",
        "        try:\n",
        "            prediction = int(clean_lines[0])\n",
        "            delegation = int(clean_lines[1])\n",
        "        except (ValueError, TypeError, IndexError):\n",
        "            prediction = None\n",
        "            delegation = None\n",
        "\n",
        "        return {\n",
        "              \"pred\": prediction,\n",
        "              \"del\": delegation\n",
        "          }\n",
        "\n",
        "    except (ValueError, IndexError):\n",
        "        print(f\"Error parsing LLM response: {response.text}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "W1wEl9aAVNmQ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769699939549,
          "user_tz": 300,
          "elapsed": 153,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "W1wEl9aAVNmQ",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# auditor (multiple LLMs: a separate LLM does the critiquing and another one does the deciding)\n",
        "def get_sequential_inference(base_scenario, model_name):\n",
        "    try:\n",
        "        # --- STEP 1: PREDICT & EXPLAIN (The Generator) ---\n",
        "        # We ask for the \"thought process\" first. This improves the prediction.\n",
        "        p1 = (f\"{base_scenario}\\n\\n\"\n",
        "              \"TASK: Predict the crowd-workers' aggregate decision. First, explain your reasoning in 1 sentence. \"\n",
        "              \"Then, conclude with 'PREDICTION: 1' (toxic) or 'PREDICTION: 0' (not toxic).\")\n",
        "\n",
        "        messages_1 = [{\"role\": \"user\", \"content\": p1}]\n",
        "\n",
        "        # OpenAI version\n",
        "        r1 = openai.chat.completions.create(model=model_name, messages=messages_1)\n",
        "        full_output = r1.choices[0].message.content.strip()\n",
        "\n",
        "        # gemini version\n",
        "        # r1 = client.models.generate_content(\n",
        "        #    model=MODEL,\n",
        "        #    contents=p1\n",
        "        # )\n",
        "\n",
        "        # 2. Extract the output (This replaces r1.choices[0].message.content)\n",
        "        # full_output = r1.text.strip()\n",
        "\n",
        "\n",
        "        import re\n",
        "        preds = re.findall(r'PREDICTION:\\s*([01])', full_output)\n",
        "        pred = preds[-1] if preds else \"0\"\n",
        "\n",
        "        # --- STEP 2: CRITIQUE (The Auditor) ---\n",
        "        # NEW CHAT: A fresh perspective to find flaws in the logic above.\n",
        "        p2 = (f\"SCENARIO: {base_scenario}\\n\\n\"\n",
        "              f\"PROPOSED LOGIC: {full_output}\\n\\n\"\n",
        "              \"TASK: Critique this reasoning (1 sentence).\")\n",
        "\n",
        "        messages_2 = [{\"role\": \"user\", \"content\": p2}]\n",
        "\n",
        "        # OpenAI version\n",
        "        r2 = openai.chat.completions.create(model=model_name, messages=messages_2)\n",
        "        critique = r2.choices[0].message.content.strip()\n",
        "\n",
        "        # gemini version\n",
        "        # r2 = client.models.generate_content(\n",
        "        #    model=MODEL,\n",
        "        #    contents=p2\n",
        "        # )\n",
        "        # critique = r2.text.strip()\n",
        "\n",
        "\n",
        "        # --- STEP 3: FINAL DECISION (The Judge) ---\n",
        "        # NEW CHAT: The final decision-maker weigh the prediction vs. the critique.\n",
        "        p3 = (f\"SCENARIO: {base_scenario}\\n\"\n",
        "              f\"PREDICTION & REASONING: {full_output}\\n\"\n",
        "              f\"CRITIQUE: {critique}\\n\\n\"\n",
        "              \"TASK: Implement the prediction (0) or delegate to the crowd-workers (1). The ground truth is the crowd-workers' aggregate decision.\\n\"\n",
        "              \"Respond with the digit ONLY.\")\n",
        "\n",
        "        messages_3 = [{\"role\": \"user\", \"content\": p3}]\n",
        "\n",
        "        # OpenAI version\n",
        "        r3 = openai.chat.completions.create(model=model_name, messages=messages_3)\n",
        "        delegate = r3.choices[0].message.content.strip()\n",
        "\n",
        "        # gemini version\n",
        "        # r3 = client.models.generate_content(\n",
        "        #    model=MODEL,\n",
        "        #    contents=p3\n",
        "        # )\n",
        "        # delegate = r3.text.strip()\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"full_thought\": full_output,\n",
        "            \"pred\": pred,\n",
        "            \"critique\": critique,\n",
        "            \"del\": delegate\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"full_thought\": str(e), \"pred\": \"Err\", \"critique\": \"Err\", \"del\": \"1\"}"
      ],
      "metadata": {
        "id": "gCK5AgWSVi2c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769700005519,
          "user_tz": 300,
          "elapsed": 176,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "gCK5AgWSVi2c",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_sequential_inference(create_prompt_base(data.iloc[0]), MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP_fDNILV0ZY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769700044629,
          "user_tz": 300,
          "elapsed": 22053,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "25fb89bf-fa85-42ae-cf99-44f9e4e0c9a2"
      },
      "id": "XP_fDNILV0ZY",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'full_thought': 'Reasoning: The comment is a civil critique about the usefulness of a mathematical analogy in an encyclopedia article and seeks input, with no insults or threats. \\nPREDICTION: 0',\n",
              " 'pred': '0',\n",
              " 'critique': 'Reasoning is largely correct: the comment is civil and contains no insults or threats, so predicting non-toxic is appropriate; however, the phrase “sounds arbitrary and ad hoc” is a harsh evaluative remark that could be perceived as negative, suggesting a more nuanced label might be warranted.',\n",
              " 'del': '0'}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "\n",
        "\n",
        "# THE MULTI-THREADED RUNNER\n",
        "def call_llm(idx, row):\n",
        "\n",
        "    ########################\n",
        "    # DECIDE method to run #\n",
        "    ########################\n",
        "    # method to run\n",
        "    # TEMP_METHOD = \"base\"\n",
        "    # TEMP_METHOD = \"sft\"\n",
        "    TEMP_METHOD = \"auditor\"\n",
        "\n",
        "    # base\n",
        "    if TEMP_METHOD == \"base\":\n",
        "\n",
        "      base = create_prompt_base(row)\n",
        "\n",
        "      # Run the base LLM\n",
        "      result = get_llm_base(base, MODEL) # standard call\n",
        "\n",
        "      # 3. Save everything back to a copy of the row\n",
        "      row_copy = row.copy()\n",
        "      row_copy['prompt'] = base # the prompt\n",
        "      row_copy['llm_prediction'] = result['pred']\n",
        "      row_copy['llm_delegate'] = result['del']\n",
        "      row_copy['human_response'] = row['toxicity']  # Ground truth for accuracy\n",
        "      row_copy['method'] = TEMP_METHOD # avoid mixing up methods\n",
        "\n",
        "      return row_copy\n",
        "\n",
        "\n",
        "\n",
        "    # auditor\n",
        "    if TEMP_METHOD == \"auditor\":\n",
        "\n",
        "      base = create_prompt_base(row)\n",
        "\n",
        "      # Run the 3-step Metacognitive Loop\n",
        "      # This now returns: pred, full_thought, critique, del\n",
        "      # result = get_sequential_inference(base, MODEL) # different llms act as auditors\n",
        "      result = get_sequential_inference(base, MODEL) # LLM is its own auditor\n",
        "\n",
        "      # Save everything back to a copy of the row\n",
        "      row_copy = row.copy()\n",
        "      row_copy['prompt'] = base # the prompt\n",
        "      row_copy['llm_full_thought'] = result['full_thought'] # The Reasoning + Prediction\n",
        "      row_copy['llm_prediction'] = result['pred']         # Extracted digit from Step 1\n",
        "      row_copy['llm_critique'] = result['critique']       # The Auditor's critique\n",
        "      row_copy['llm_delegate'] = result['del']           # The Judge's final decision\n",
        "      row_copy['human_response'] = row['toxicity']         # Ground truth for accuracy\n",
        "      row_copy['method'] = TEMP_METHOD # avoid mixing up methods\n",
        "\n",
        "      return row_copy\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W_ven7N5WSZt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769701878294,
          "user_tz": 300,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "W_ven7N5WSZt",
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get sample df (that wasn't used to train the random forest)\n",
        "N_SAMPLES = 50\n",
        "sampled_rows = data_agg.sample(n=N_SAMPLES) # can't have a random state, or we'll do the same values over and over!\n",
        "\n",
        "# initialize results\n",
        "results = []\n",
        "\n",
        "# make call\n",
        "with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    futures = [executor.submit(call_llm, idx, row) for idx, row in sampled_rows.iterrows()]\n",
        "    for f in as_completed(futures):\n",
        "        results.append(f.result())\n",
        "\n",
        "# save\n",
        "df_results = pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "AKxoHWuJWdoK",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769702073844,
          "user_tz": 300,
          "elapsed": 193454,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "AKxoHWuJWdoK",
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "# write file; add timestamp\n",
        "df_results['timestamp'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "###########################################\n",
        "# MAKE SURE THE FILE MATCHES TEMP_METHOD! #\n",
        "###########################################\n",
        "# path = 'gs://exceptions-data/LLM Delegation/Wikipedia Toxicity/Results/base_' + MODEL + '.csv'\n",
        "path = 'gs://exceptions-data/LLM Delegation/Wikipedia Toxicity/Results/auditor_' + MODEL + '.csv'\n",
        "\n",
        "# Load, append, and re-save\n",
        "try:\n",
        "    df_existing = pd.read_csv(path)\n",
        "    df_results = pd.concat([df_existing, df_results], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    pass\n",
        "\n",
        "df_results.to_csv(path, index=False)"
      ],
      "metadata": {
        "id": "Bi3naWSUWijj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1769702073976,
          "user_tz": 300,
          "elapsed": 153,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "Bi3naWSUWijj",
      "execution_count": 106,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "wikipedia toxicity"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}