{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformer-lens\n",
        "# !pip install --upgrade transformers tokenizers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnCFywTNi8i5",
        "outputId": "03cb4afd-fed3-4c74-ca15-560a875393fd"
      },
      "id": "lnCFywTNi8i5",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformer-lens in /usr/local/lib/python3.12/dist-packages (2.17.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (1.12.0)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (4.0.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.8.1)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.36.0)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.3.7)\n",
            "Requirement already satisfied: numpy<2,>=1.26 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (2.2.2)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (6.33.4)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.2.1)\n",
            "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.51 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (4.57.6)\n",
            "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.0.5)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (4.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (4.15.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.12/dist-packages (from transformer-lens) (0.24.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer-lens) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer-lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer-lens) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer-lens) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer-lens) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer-lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer-lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer-lens) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer-lens) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer-lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformer-lens) (1.2.0)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping>=0.2.11->transformer-lens) (0.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer-lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer-lens) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer-lens) (2025.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer-lens) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer-lens) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer-lens) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer-lens) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer-lens) (0.22.2)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer-lens) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer-lens) (3.1.46)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer-lens) (4.5.1)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer-lens) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer-lens) (2.49.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens) (3.13.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer-lens) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer-lens) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer-lens) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.6->transformer-lens) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.6->transformer-lens) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer-lens) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load smallest model with eye towards speed (clearing existing memory, etc.)"
      ],
      "metadata": {
        "id": "22EdNextzAbu"
      },
      "id": "22EdNextzAbu"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformer_lens import HookedTransformer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Choose ONE model. 0.6B is recommended for speed/testing.\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\" # Qwen-3 (0.6B class)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"üöÄ Loading {model_name} on {device}...\")\n",
        "\n",
        "# 1. Clear any leftover memory from previous attempts\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 2. Load the model ONCE with memory-saving settings\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16, # Half precision saves 50% VRAM\n",
        "    device_map=\"auto\"          # Handles GPU placement automatically\n",
        ")\n",
        "\n",
        "# 3. Hook it for interpretability (SAE work)\n",
        "_llm = HookedTransformer.from_pretrained(\n",
        "    model_name,\n",
        "    hf_model=hf_model,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"‚ú® Success! Model is hooked and ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtALs72C2lTy",
        "outputId": "89b21b02-2862-4ba2-d1e6-3398d617e1ac"
      },
      "id": "AtALs72C2lTy",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Loading Qwen/Qwen2.5-0.5B-Instruct on cpu...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model Qwen/Qwen2.5-0.5B-Instruct into HookedTransformer\n",
            "‚ú® Success! Model is hooked and ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load lending club data. To ensure balance, merge 100k accepted and 100k rejected loans."
      ],
      "metadata": {
        "id": "3oBdI6tYzHyi"
      },
      "id": "3oBdI6tYzHyi"
    },
    {
      "cell_type": "code",
      "source": [
        "# load lending club accepted data\n",
        "import pandas as pd\n",
        "df_accepted = pd.read_csv(\"gs://exceptions-data/LLM Delegation/LendingClub/data/accepted_2007_to_2018Q4.csv\",\n",
        "                          nrows = 100000) # just first 100k rows, for speed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# load lending club rejected data\n",
        "import pandas as pd\n",
        "df_rejected = pd.read_csv(\"gs://exceptions-data/LLM Delegation/LendingClub/data/rejected_2007_to_2018Q4.csv\",\n",
        "                          nrows = 100000) # just first 100k rows, for speed\n",
        "\n",
        "\n",
        "\n",
        "# combine data frames\n",
        "# only these columns are shared across the two dfs\n",
        "# zip code is also shared, but partially excluded (only three digits)\n",
        "df = pd.concat([\n",
        "    df_accepted.rename(columns={\n",
        "        \"loan_amnt\": \"loan_amnt\",\n",
        "        \"title\": \"title\",\n",
        "        \"addr_state\": \"addr_state\",\n",
        "        \"emp_length\": \"emp_length\",\n",
        "        \"policy_code\": \"policy_code\",\n",
        "    })[\n",
        "        [\"loan_amnt\", \"title\", \"addr_state\", \"emp_length\", \"policy_code\"]\n",
        "    ].assign(accepted=1),\n",
        "\n",
        "    df_rejected.rename(columns={\n",
        "        \"Amount Requested\": \"loan_amnt\",\n",
        "        \"Loan Title\": \"title\",\n",
        "        \"State\": \"addr_state\",\n",
        "        \"Employment Length\": \"emp_length\",\n",
        "        \"Policy Code\": \"policy_code\",\n",
        "    })[\n",
        "        [\"loan_amnt\", \"title\", \"addr_state\", \"emp_length\", \"policy_code\"]\n",
        "    ].assign(accepted=0)\n",
        "], ignore_index=True)\n",
        "\n",
        "\n",
        "# clean title column\n",
        "df[\"title\"] = df[\"title\"].str.lower().str.replace(\"_\", \" \", regex=False)\n",
        "\n",
        "# clean length column\n",
        "df[\"emp_length\"] = df[\"emp_length\"].map({\"< 1 year\":0, \"1 year\":1, \"2 years\":2, \"3 years\":3, \"4 years\":4, \"5 years\":5, \"6 years\":6, \"7 years\":7, \"8 years\":8, \"9 years\":9, \"10+ years\":10})\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "r2gJa_SIZ0xU",
        "outputId": "2a795b65-be34-4570-a931-729512e16cec"
      },
      "id": "r2gJa_SIZ0xU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2639878748.py:3: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_accepted = pd.read_csv(\"gs://exceptions-data/LLM Delegation/LendingClub/data/accepted_2007_to_2018Q4.csv\",\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
              "0  68407277        NaN     3600.0       3600.0           3600.0   36 months   \n",
              "1  68355089        NaN    24700.0      24700.0          24700.0   36 months   \n",
              "2  68341763        NaN    20000.0      20000.0          20000.0   60 months   \n",
              "3  66310712        NaN    35000.0      35000.0          35000.0   60 months   \n",
              "4  68476807        NaN    10400.0      10400.0          10400.0   60 months   \n",
              "\n",
              "   int_rate  installment grade sub_grade  ... hardship_payoff_balance_amount  \\\n",
              "0     13.99       123.03     C        C4  ...                            NaN   \n",
              "1     11.99       820.28     C        C1  ...                            NaN   \n",
              "2     10.78       432.66     B        B4  ...                            NaN   \n",
              "3     14.85       829.90     C        C5  ...                            NaN   \n",
              "4     22.45       289.91     F        F1  ...                            NaN   \n",
              "\n",
              "  hardship_last_payment_amount disbursement_method  debt_settlement_flag  \\\n",
              "0                          NaN                Cash                     N   \n",
              "1                          NaN                Cash                     N   \n",
              "2                          NaN                Cash                     N   \n",
              "3                          NaN                Cash                     N   \n",
              "4                          NaN                Cash                     N   \n",
              "\n",
              "  debt_settlement_flag_date settlement_status settlement_date  \\\n",
              "0                       NaN               NaN             NaN   \n",
              "1                       NaN               NaN             NaN   \n",
              "2                       NaN               NaN             NaN   \n",
              "3                       NaN               NaN             NaN   \n",
              "4                       NaN               NaN             NaN   \n",
              "\n",
              "  settlement_amount settlement_percentage settlement_term  \n",
              "0               NaN                   NaN             NaN  \n",
              "1               NaN                   NaN             NaN  \n",
              "2               NaN                   NaN             NaN  \n",
              "3               NaN                   NaN             NaN  \n",
              "4               NaN                   NaN             NaN  \n",
              "\n",
              "[5 rows x 151 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50479f36-5c6b-46f5-be23-cbdbafd97e59\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>member_id</th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>funded_amnt</th>\n",
              "      <th>funded_amnt_inv</th>\n",
              "      <th>term</th>\n",
              "      <th>int_rate</th>\n",
              "      <th>installment</th>\n",
              "      <th>grade</th>\n",
              "      <th>sub_grade</th>\n",
              "      <th>...</th>\n",
              "      <th>hardship_payoff_balance_amount</th>\n",
              "      <th>hardship_last_payment_amount</th>\n",
              "      <th>disbursement_method</th>\n",
              "      <th>debt_settlement_flag</th>\n",
              "      <th>debt_settlement_flag_date</th>\n",
              "      <th>settlement_status</th>\n",
              "      <th>settlement_date</th>\n",
              "      <th>settlement_amount</th>\n",
              "      <th>settlement_percentage</th>\n",
              "      <th>settlement_term</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>68407277</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3600.0</td>\n",
              "      <td>3600.0</td>\n",
              "      <td>3600.0</td>\n",
              "      <td>36 months</td>\n",
              "      <td>13.99</td>\n",
              "      <td>123.03</td>\n",
              "      <td>C</td>\n",
              "      <td>C4</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cash</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>68355089</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24700.0</td>\n",
              "      <td>24700.0</td>\n",
              "      <td>24700.0</td>\n",
              "      <td>36 months</td>\n",
              "      <td>11.99</td>\n",
              "      <td>820.28</td>\n",
              "      <td>C</td>\n",
              "      <td>C1</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cash</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>68341763</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>10.78</td>\n",
              "      <td>432.66</td>\n",
              "      <td>B</td>\n",
              "      <td>B4</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cash</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>66310712</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35000.0</td>\n",
              "      <td>35000.0</td>\n",
              "      <td>35000.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>14.85</td>\n",
              "      <td>829.90</td>\n",
              "      <td>C</td>\n",
              "      <td>C5</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cash</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>68476807</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10400.0</td>\n",
              "      <td>10400.0</td>\n",
              "      <td>10400.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>22.45</td>\n",
              "      <td>289.91</td>\n",
              "      <td>F</td>\n",
              "      <td>F1</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cash</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 151 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50479f36-5c6b-46f5-be23-cbdbafd97e59')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-50479f36-5c6b-46f5-be23-cbdbafd97e59 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-50479f36-5c6b-46f5-be23-cbdbafd97e59');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_accepted"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base prompt for LLM engagement."
      ],
      "metadata": {
        "id": "DqU_tklFzVHq"
      },
      "id": "DqU_tklFzVHq"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# scenario builder\n",
        "def create_prompt_base(row):\n",
        "\n",
        "    loan_amnt = int(round(row['loan_amnt']))\n",
        "    emp_length = int(round(row['emp_length']))\n",
        "\n",
        "    base = (\n",
        "        f\"Someone from {row['addr_state']} is applying for a loan of \"\n",
        "        f\"${loan_amnt}.\"\n",
        "    )\n",
        "\n",
        "    if pd.notna(row['title']):\n",
        "        base += f\" The loan is for a {row['title']}.\"\n",
        "\n",
        "    base += f\" They have been employed for {emp_length} years.\"\n",
        "\n",
        "    return base\n",
        "\n",
        "create_prompt_base(df.iloc[0])"
      ],
      "metadata": {
        "id": "GJ6K7eavZ_1X"
      },
      "id": "GJ6K7eavZ_1X",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low level runner for all LLM calls."
      ],
      "metadata": {
        "id": "bYGzrXdPzbXq"
      },
      "id": "bYGzrXdPzbXq"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "\n",
        "MAX_CTX = 512 # Lowering this slightly further ensures speed\n",
        "RESERVE = 8\n",
        "\n",
        "def get_llm_base(prompt: str, max_tokens: int = 20):\n",
        "    \"\"\"\n",
        "    Optimized LLM call with variable length control.\n",
        "    \"\"\"\n",
        "    global _llm\n",
        "    device = next(_llm.parameters()).device\n",
        "\n",
        "    # Tokenize and Truncate\n",
        "    toks = _llm.to_tokens(prompt, prepend_bos=False).to(device)\n",
        "    if toks.shape[1] > MAX_CTX - RESERVE:\n",
        "        toks = toks[:, -(MAX_CTX - RESERVE):]\n",
        "\n",
        "    # Only cache the layer we need for the SAE to save memory\n",
        "    target_hook = f\"blocks.{LAYER}.mlp.hook_post\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, cache = _llm.run_with_cache(\n",
        "            toks,\n",
        "            names_filter=lambda name: name == target_hook\n",
        "        )\n",
        "\n",
        "        # Generate using the dynamic max_tokens value\n",
        "        out = _llm.generate(\n",
        "            toks,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False,\n",
        "            stop_at_eos=True,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "    # Extract and Clean\n",
        "    gen_tokens = out[0][toks.shape[1]:]\n",
        "    text = _llm.to_string(gen_tokens)\n",
        "    text = re.sub(r\"<\\|im_end\\|>|<\\|endoftext\\|>|<\\|im_start\\|>|assistant|user\", \"\", text).strip()\n",
        "\n",
        "    return {\n",
        "        \"cache\": cache,\n",
        "        \"tokens\": toks,\n",
        "        \"text\": text,\n",
        "    }"
      ],
      "metadata": {
        "id": "Ilqkh8pZxDoO"
      },
      "id": "Ilqkh8pZxDoO",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions that will extract the final decision, and either provide supporting logic or a critique."
      ],
      "metadata": {
        "id": "dzB6iE8Pze8T"
      },
      "id": "dzB6iE8Pze8T"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def run_final_decision(prompt: str, max_tokens: int = 1):\n",
        "    r = get_llm_base(prompt, max_tokens=max_tokens)\n",
        "\n",
        "    generated_text = r.get(\"text\", \"\").lower() # Normalize to lowercase\n",
        "\n",
        "    # Updated regex to find 'accept' or 'reject'\n",
        "    m = re.findall(r'\\b(accept|reject)\\b', generated_text)\n",
        "    decision = m[-1] if m else None\n",
        "\n",
        "    return {\n",
        "        \"del\": decision, # This will now be 'accept' or 'reject'\n",
        "        \"cache\": r[\"cache\"],\n",
        "        \"tokens\": r[\"tokens\"],\n",
        "        \"text\": generated_text,\n",
        "        \"final_prompt\": prompt,\n",
        "    }\n",
        "\n",
        "# --- Supporting Logic ---\n",
        "\n",
        "def get_llm_base_support(base_scenario: str):\n",
        "    # Step 1: Prediction (More tokens for context)\n",
        "    pred_prompt = (\n",
        "        f\"{base_scenario}\\n\\n\"\n",
        "        \"Accept or reject the loan.\\n\"\n",
        "        \"Initial Decision (accept or reject):\"\n",
        "    )\n",
        "    prediction = get_llm_base(pred_prompt, max_tokens=1).get(\"text\", \"\").strip()\n",
        "\n",
        "    # Step 2: Support (More tokens for reasoning)\n",
        "    support_prompt = (\n",
        "        f\"SCENARIO: {base_scenario}\\n\"\n",
        "        f\"INITIAL DECISION: {prediction}\\n\\n\"\n",
        "        \"Provide one more reason why this decision is CORRECT.\\n\"\n",
        "        \"Sentence: This is potentially because\"\n",
        "    )\n",
        "    support_text = get_llm_base(support_prompt, max_tokens=50).get(\"text\", \"\").strip()\n",
        "    support = \"This is potentially because \" + support_text\n",
        "\n",
        "    # Step 3: Final Decision (Short and focused)\n",
        "    final_prompt = (\n",
        "        f\"SCENARIO: {base_scenario}\\n\"\n",
        "        f\"INITIAL DECISION: {prediction}\\n\"\n",
        "        f\"SUPPORT OF INITIAL DECISION: {support}\\n\\n\"\n",
        "        \"Final Decision (accept or reject):\"\n",
        "    )\n",
        "    out = run_final_decision(final_prompt, max_tokens=1)\n",
        "    out.update({\"prediction\": prediction, \"support\": support})\n",
        "    return out\n",
        "\n",
        "\n",
        "# --- Critique ---\n",
        "\n",
        "def get_sequential_inference(base_scenario: str):\n",
        "    # Step 1: Prediction\n",
        "    pred_prompt = (\n",
        "        f\"{base_scenario}\\n\\n\"\n",
        "        \"Accept or reject the loan.\\n\"\n",
        "        \"Initial Decision (accept or reject):\"\n",
        "    )\n",
        "    prediction = get_llm_base(pred_prompt, max_tokens=1).get(\"text\", \"\").strip()\n",
        "\n",
        "    # Step 2: Critique (Forced pivot with 'However')\n",
        "    critique_prompt = (\n",
        "        f\"SCENARIO: {base_scenario}\\n\"\n",
        "        f\"INITIAL DECISION: {prediction}\\n\\n\"\n",
        "        \"Provide one reason why this decision is INCORRECT.\\n\"\n",
        "        \"Sentence: On the other hand ,\"\n",
        "    )\n",
        "    critique_text = get_llm_base(critique_prompt, max_tokens=50).get(\"text\", \"\").strip()\n",
        "    critique = \"On the other hand, \" + critique_text\n",
        "\n",
        "    # Step 3: Final Decision\n",
        "    final_prompt = (\n",
        "        f\"SCENARIO: {base_scenario}\\n\"\n",
        "        f\"INITIAL DECISION: {prediction}\\n\"\n",
        "        f\"CRITIQUE OF INITIAL DECISION: {critique}\\n\\n\"\n",
        "        \"Final Decision (accept or reject):\"\n",
        "    )\n",
        "    out = run_final_decision(final_prompt, max_tokens=1)\n",
        "    out.update({\"prediction\": prediction, \"critique\": critique})\n",
        "    return out"
      ],
      "metadata": {
        "id": "cOAWAAmJrxAq"
      },
      "id": "cOAWAAmJrxAq",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train SAE and collect performance data."
      ],
      "metadata": {
        "id": "EYFSjBHVznVy"
      },
      "id": "EYFSjBHVznVy"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "N_SAMPLES = 10    # Total samples to collect\n",
        "LAYER = 10        # Probing Layer\n",
        "SAE_STEPS = 150   # Training steps\n",
        "MAX_CTX = 512     # Reduced for VRAM safety\n",
        "RESERVE = 16\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.empty_cache() # Clear any ghost memory\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self, d_in, d_hidden):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Linear(d_in, d_hidden)\n",
        "        self.dec = nn.Linear(d_hidden, d_in, bias=False)\n",
        "    def forward(self, x):\n",
        "        z = F.relu(self.enc(x))\n",
        "        return self.dec(z), z\n",
        "\n",
        "def truncate_to_ctx(prompt: str) -> str:\n",
        "    toks = _llm.to_tokens(prompt, prepend_bos=False)\n",
        "    if toks.shape[1] <= MAX_CTX - RESERVE: return prompt\n",
        "    return _llm.to_string(toks[0, -(MAX_CTX - RESERVE):])\n",
        "\n",
        "def decision_activation(result, layer):\n",
        "    hook_name = f\"blocks.{layer}.mlp.hook_post\"\n",
        "    return result[\"cache\"][hook_name][0, -1]\n",
        "\n",
        "@torch.no_grad()\n",
        "def sae_stats(Xpart, X_mean, X_std, sae_model):\n",
        "    Xp = (Xpart - X_mean) / X_std\n",
        "    _, z = sae_model(Xp)\n",
        "    l1 = z.abs().sum(dim=1).mean().item()\n",
        "    active = (z > 0).float().mean(dim=1).mean().item()\n",
        "    return l1, active\n",
        "\n",
        "# ---------------- COLLECTION LOOP ----------------\n",
        "\n",
        "base_X, audit_X = [], []\n",
        "results_metadata = [] # Store text and ground truth here\n",
        "print(f\"üöÄ Starting collection: Targeting {N_SAMPLES} samples...\")\n",
        "\n",
        "while len(base_X) < N_SAMPLES:\n",
        "    row = df.sample(1).iloc[0]\n",
        "    if pd.isna(row[\"emp_length\"]): continue\n",
        "\n",
        "    # Capture Ground Truth (Historical Data)\n",
        "    ground_truth = \"accept\" if row[\"accepted\"] == 1 else \"reject\"\n",
        "    scenario = truncate_to_ctx(create_prompt_base(row))\n",
        "\n",
        "    b_res = get_llm_base_support(scenario)\n",
        "    a_res = get_sequential_inference(scenario)\n",
        "\n",
        "    if b_res[\"del\"] and a_res[\"del\"]:\n",
        "        base_X.append(decision_activation(b_res, LAYER).detach().cpu())\n",
        "        audit_X.append(decision_activation(a_res, LAYER).detach().cpu())\n",
        "\n",
        "        # Track accuracy metadata\n",
        "        results_metadata.append({\n",
        "            \"ground_truth\": ground_truth,\n",
        "            \"base_decision\": b_res[\"del\"],\n",
        "            \"audit_decision\": a_res[\"del\"]\n",
        "        })\n",
        "\n",
        "        print(f\"‚úÖ Sample {len(base_X)}/{N_SAMPLES} | Actual: {ground_truth} | Base: {b_res['del']} | Audit: {a_res['del']}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Skip | B: '{b_res['text']}' | A: '{a_res['text']}'\")\n",
        "\n",
        "# Convert activations to tensors\n",
        "base_X = torch.stack(base_X).float().to(device)\n",
        "audit_X = torch.stack(audit_X).float().to(device)\n",
        "\n",
        "\n",
        "# ---------------- 2. TRAIN SAE ----------------\n",
        "\n",
        "d_in = X.shape[1]\n",
        "sae = SAE(d_in, 2 * d_in).to(device)\n",
        "opt = torch.optim.AdamW(sae.parameters(), lr=1e-3)\n",
        "\n",
        "X_mean, X_std = X.mean(0), X.std(0) + 1e-6\n",
        "Xn = (X - X_mean) / X_std\n",
        "\n",
        "print(\"\\nüèãÔ∏è Training SAE...\")\n",
        "for step in range(SAE_STEPS):\n",
        "    x_hat, z = sae(Xn)\n",
        "    l1_loss = z.abs().mean()\n",
        "    active_pct = (z > 0).float().mean().item() * 100\n",
        "    loss = F.mse_loss(x_hat, Xn) + 5e-4 * l1_loss\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(f\"Step {step:3} | Loss: {loss.item():.4f} | L1: {l1_loss:.2f} | Active: {active_pct:.1f}%\")\n",
        "\n",
        "# ---------------- 3. FINAL EVALUATION ----------------\n",
        "\n",
        "base_l1, base_active = sae_stats(base_X, X_mean, X_std, sae)\n",
        "audit_l1, audit_active = sae_stats(audit_X, X_mean, X_std, sae)\n",
        "\n",
        "print(f\"\\n‚ú® FINAL STATS (Layer {LAYER})\")\n",
        "print(f\"L1 (Density):  Base={base_l1:.2f} | Audit={audit_l1:.2f}\")\n",
        "print(f\"Active Features: Base={base_active*100:.1f}% | Audit={audit_active*100:.1f}%\")\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- 4. ACCURACY ANALYSIS ----------------\n",
        "\n",
        "base_correct = sum(1 for m in results_metadata if m[\"base_decision\"] == m[\"ground_truth\"])\n",
        "audit_correct = sum(1 for m in results_metadata if m[\"audit_decision\"] == m[\"ground_truth\"])\n",
        "\n",
        "base_acc = (base_correct / N_SAMPLES) * 100\n",
        "audit_acc = (audit_correct / N_SAMPLES) * 100\n",
        "\n",
        "print(f\"\\nüéØ ACCURACY REPORT\")\n",
        "print(f\"Base Accuracy (Support):  {base_acc:.1f}%\")\n",
        "print(f\"Audit Accuracy (Critique): {audit_acc:.1f}%\")\n",
        "print(f\"Accuracy Delta:           {audit_acc - base_acc:+.1f}%\")\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- 5. PCA ANALYSIS (RECOVER PC1) ----------------\n",
        "\n",
        "print(\"\\nüîç Extracting Principal Components...\")\n",
        "\n",
        "# Center the data for PCA\n",
        "X_centered = X - X.mean(dim=0)\n",
        "\n",
        "# U: Left singular vectors, S: Singular values, V: Principal Components\n",
        "U, S, V = torch.pca_lowrank(X_centered, q=2)\n",
        "\n",
        "# PC1 is the first column of V\n",
        "pc1 = V[:, 0]\n",
        "\n",
        "# Project the Base and Audit activations onto PC1\n",
        "base_projections = base_X @ pc1\n",
        "audit_projections = audit_X @ pc1\n",
        "\n",
        "print(f\"PC1 Explained Variance: {(S[0]**2 / torch.sum(S**2)) * 100:.1f}%\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Mean PC1 Projection (Base):  {base_projections.mean().item():.4f}\")\n",
        "print(f\"Mean PC1 Projection (Audit): {audit_projections.mean().item():.4f}\")\n",
        "\n",
        "# Calculate the 'Separation' (How much PC1 distinguishes the two paths)\n",
        "separation = (base_projections.mean() - audit_projections.mean()).abs()\n",
        "print(f\"Path Separation on PC1:      {separation.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-dT7sM1cVkI",
        "outputId": "6c17d9ec-12e5-4bb3-f4ea-050fc2eda61e"
      },
      "id": "C-dT7sM1cVkI",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting collection: Targeting 10 samples...\n",
            "‚úÖ Sample 1/10 | Actual: accept | Base: accept | Audit: accept\n",
            "‚úÖ Sample 2/10 | Actual: reject | Base: accept | Audit: reject\n",
            "‚úÖ Sample 3/10 | Actual: reject | Base: accept | Audit: accept\n",
            "‚úÖ Sample 4/10 | Actual: reject | Base: accept | Audit: reject\n",
            "‚úÖ Sample 5/10 | Actual: accept | Base: accept | Audit: accept\n",
            "‚úÖ Sample 6/10 | Actual: accept | Base: accept | Audit: reject\n",
            "‚úÖ Sample 7/10 | Actual: accept | Base: accept | Audit: reject\n",
            "‚úÖ Sample 8/10 | Actual: reject | Base: accept | Audit: reject\n",
            "‚úÖ Sample 9/10 | Actual: accept | Base: accept | Audit: reject\n",
            "‚úÖ Sample 10/10 | Actual: reject | Base: accept | Audit: reject\n",
            "\n",
            "üèãÔ∏è Training SAE...\n",
            "Step   0 | Loss: 0.9994 | L1: 0.22 | Active: 50.1%\n",
            "Step  50 | Loss: 0.0185 | L1: 0.65 | Active: 52.7%\n",
            "Step 100 | Loss: 0.0011 | L1: 0.61 | Active: 51.1%\n",
            "\n",
            "‚ú® FINAL STATS (Layer 10)\n",
            "L1 (Density):  Base=7763.28 | Audit=6606.15\n",
            "Active Features: Base=64.0% | Audit=54.3%\n",
            "\n",
            "üéØ ACCURACY REPORT\n",
            "Base Accuracy (Support):  50.0%\n",
            "Audit Accuracy (Critique): 60.0%\n",
            "Accuracy Delta:           +10.0%\n",
            "\n",
            "üîç Extracting Principal Components...\n",
            "PC1 Explained Variance: 70.1%\n",
            "------------------------------\n",
            "Mean PC1 Projection (Base):  -0.1646\n",
            "Mean PC1 Projection (Audit): 1.4508\n",
            "Path Separation on PC1:      1.6153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm that SAE loop correctly prompted models."
      ],
      "metadata": {
        "id": "v4wzGhAlzshl"
      },
      "id": "v4wzGhAlzshl"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Grab a single sample\n",
        "test_row = df.sample(1).iloc[0]\n",
        "test_scenario = truncate_to_ctx(create_prompt_base(test_row))\n",
        "\n",
        "print(\"--- üõ†Ô∏è DRY RUN: LOGIC VERIFICATION ---\")\n",
        "\n",
        "# Print the input scenario first\n",
        "print(f\"\\n[ORIGINAL SCENARIO]\\n{test_scenario}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n[PATH A: BASE]\")\n",
        "# This uses the 60-token reasoning we just set up\n",
        "res_support = get_llm_base_support(test_scenario)\n",
        "print(f\"INITIAL DECISION: {res_support['prediction']}\")\n",
        "print(f\"SUPPORT:    {res_support['support']}\")\n",
        "print(f\"FINAL DECISION:   {res_support['del']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "\n",
        "print(\"\\n[PATH B: AUDITOR]\")\n",
        "# This uses the 'However,' pivot to ensure a real critique\n",
        "res_critique = get_sequential_inference(test_scenario)\n",
        "print(f\"INITIAL DECISION: {res_critique['prediction']}\")\n",
        "print(f\"CRITIQUE:   {res_critique['critique']}\")\n",
        "print(f\"FINAL DECISION:   {res_critique['del']}\")\n",
        "\n",
        "print(\"\\n--- ‚úÖ CHECK COMPLETE ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etIO9SsBXwSa",
        "outputId": "f11e9d5c-603d-4557-c5f9-0f3176b4685d"
      },
      "id": "etIO9SsBXwSa",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üõ†Ô∏è DRY RUN: LOGIC VERIFICATION ---\n",
            "\n",
            "[ORIGINAL SCENARIO]\n",
            "Someone from MD is applying for a loan of $12000. The loan is for a debt consolidation. They have been employed for 10 years.\n",
            "----------------------------------------\n",
            "\n",
            "[PATH A: BASE]\n",
            "INITIAL DECISION: Accept\n",
            "SUPPORT:    This is potentially because the person has been employed for 10 years, which is a long time for a person to be employed. The person has been employed for 10 years, which is a long time for a person to be employed. The person has been\n",
            "FINAL DECISION:   accept\n",
            "\n",
            "========================================\n",
            "\n",
            "[PATH B: AUDITOR]\n",
            "INITIAL DECISION: Accept\n",
            "CRITIQUE:   On the other hand, the decision to accept the loan is incorrect because the applicant has been employed for 10 years, which is a significant amount of time in the financial industry. The applicant's employment history suggests that they have a strong credit history and a good track record\n",
            "FINAL DECISION:   reject\n",
            "\n",
            "--- ‚úÖ CHECK COMPLETE ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try 'steering' the base model using the auditor activations."
      ],
      "metadata": {
        "id": "m75YAIeJz8rL"
      },
      "id": "m75YAIeJz8rL"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. PARAMETERS ---\n",
        "N_TEST = 10\n",
        "\n",
        "# --- 1. SETUP STEERING ---\n",
        "# Using the vector derived from PC1\n",
        "steering_vector = (audit_X.mean(0) - base_X.mean(0)).to(device)\n",
        "COEFF = 2.0  # Strength of the internal \"Auditor\" nudge\n",
        "\n",
        "def steering_hook(value, hook):\n",
        "    return value + (COEFF * steering_vector)\n",
        "\n",
        "# --- 2. THE STEERING LOOP ---\n",
        "print(f\"üöÄ Running Steering Test (Strength: {COEFF})...\")\n",
        "\n",
        "for i in range(N_TEST):\n",
        "    row = df_clean.sample(1).iloc[0]\n",
        "    gt = 'accept' if row['accepted'] == 1 else 'reject'\n",
        "    prompt = f\"{truncate_to_ctx(create_prompt_base(row))} Respond with only one word (accept or reject):\"\n",
        "\n",
        "    def get_decision(is_steered):\n",
        "        if is_steered:\n",
        "            with _llm.hooks([(f\"blocks.{LAYER}.mlp.hook_post\", steering_hook)]):\n",
        "                out = _llm.generate(prompt, max_new_tokens=5, verbose=False)\n",
        "        else:\n",
        "            out = _llm.generate(prompt, max_new_tokens=5, verbose=False)\n",
        "\n",
        "        # Slicing logic you just perfected\n",
        "        text = out[0] if isinstance(out, list) else out\n",
        "        comp = text[len(prompt):].strip().lower()\n",
        "\n",
        "        if \"accept\" in comp: return \"accept\"\n",
        "        if \"reject\" in comp: return \"reject\"\n",
        "        return \"unknown\"\n",
        "\n",
        "    # Run both passes\n",
        "    base_dec = get_decision(is_steered=False)\n",
        "    steer_dec = get_decision(is_steered=True)\n",
        "\n",
        "    status = \"üöÄ FLIP!\" if base_dec != steer_dec else \"-\"\n",
        "    if steer_dec == gt and base_dec != gt: status = \"‚ú® CORRECTIVE FLIP!\"\n",
        "\n",
        "    print(f\"\\nSample {i+1} | GT: {gt:6}\")\n",
        "    print(f\"  Base:    {base_dec:6}\")\n",
        "    print(f\"  Steered: {steer_dec:6} | {status}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNoi3crpuzmo",
        "outputId": "6e91f4ec-f728-4ca8-d3f8-fb0437967a3d"
      },
      "id": "PNoi3crpuzmo",
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Running Steering Test (Strength: 3.0)...\n",
            "\n",
            "Sample 1 | GT: reject\n",
            "  Base:    unknown\n",
            "  Steered: accept | üöÄ FLIP!\n",
            "\n",
            "Sample 2 | GT: reject\n",
            "  Base:    unknown\n",
            "  Steered: accept | üöÄ FLIP!\n",
            "\n",
            "Sample 3 | GT: accept\n",
            "  Base:    unknown\n",
            "  Steered: accept | ‚ú® CORRECTIVE FLIP!\n",
            "\n",
            "Sample 4 | GT: accept\n",
            "  Base:    accept\n",
            "  Steered: accept | -\n",
            "\n",
            "Sample 5 | GT: reject\n",
            "  Base:    accept\n",
            "  Steered: accept | -\n",
            "\n",
            "Sample 6 | GT: reject\n",
            "  Base:    reject\n",
            "  Steered: accept | üöÄ FLIP!\n",
            "\n",
            "Sample 7 | GT: accept\n",
            "  Base:    accept\n",
            "  Steered: accept | -\n",
            "\n",
            "Sample 8 | GT: reject\n",
            "  Base:    accept\n",
            "  Steered: accept | -\n",
            "\n",
            "Sample 9 | GT: accept\n",
            "  Base:    accept\n",
            "  Steered: accept | -\n",
            "\n",
            "Sample 10 | GT: accept\n",
            "  Base:    accept\n",
            "  Steered: accept | -\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Neural Activation (Auditor)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}