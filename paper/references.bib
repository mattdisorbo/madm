@article{elhage2022superposition,
  title     = {Toy Models of Superposition},
  author    = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal   = {Transformer Circuits Thread},
  year      = {2022},
  url       = {https://transformer-circuits.pub/2022/toy_model/index.html}
}

@article{bricken2023monosemanticity,
  title     = {Towards Monosemanticity: Decomposing Language Models with Dictionary Learning},
  author    = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and others},
  journal   = {Transformer Circuits Thread},
  year      = {2023},
  url       = {https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@article{cunningham2023sparse,
  title     = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  author    = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal   = {arXiv preprint arXiv:2309.08600},
  year      = {2023}
}

@article{olah2020zoom,
  title     = {Zoom In: An Introduction to Circuits},
  author    = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal   = {Distill},
  year      = {2020},
  doi       = {10.23915/distill.00024.001}
}

@article{turner2023activation,
  title     = {Activation Addition: Steering Language Models Without Optimization},
  author    = {Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Mini, David and Udell, Jordan Taylor and MacDiarmid, Monte and Jenner, Erik and Mickens, Asa Cooper},
  journal   = {arXiv preprint arXiv:2308.10248},
  year      = {2023}
}

@article{panickssery2023steering,
  title     = {Steering {Llama 2} via Contrastive Activation Addition},
  author    = {Panickssery, Nina and Gabrieli, Nick and Schulz, Julian and Tong, Meg and Hubinger, Evan and Turner, Alexander Matt},
  journal   = {arXiv preprint arXiv:2312.06681},
  year      = {2023}
}

@book{barocas2023fairness,
  title     = {Fairness and Machine Learning: Limitations and Opportunities},
  author    = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  publisher = {MIT Press},
  year      = {2023},
  url       = {https://fairmlbook.org}
}

@inproceedings{datta2016algorithmic,
  title     = {Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems},
  author    = {Datta, Anupam and Sen, Shayak and Zick, Yair},
  booktitle = {IEEE Symposium on Security and Privacy},
  year      = {2016}
}

@article{kadavath2022language,
  title     = {Language Models (Mostly) Know What They Know},
  author    = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal   = {arXiv preprint arXiv:2207.05221},
  year      = {2022}
}

@article{xiong2024can,
  title     = {Can {LLMs} Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in {LLMs}},
  author    = {Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
  journal   = {arXiv preprint arXiv:2306.13063},
  year      = {2024}
}

@inproceedings{geifman2017selective,
  title     = {Selective Prediction in Deep Neural Networks with Design of Experiments},
  author    = {Geifman, Yonatan and El-Yaniv, Ran},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017}
}

@article{du2023improving,
  title     = {Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author    = {Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  journal   = {arXiv preprint arXiv:2305.14325},
  year      = {2023}
}

@article{chan2023chateval,
  title     = {{ChatEval}: Towards Better {LLM}-based Evaluators through Multi-Agent Debate},
  author    = {Chan, Chi-Min and Chen, Weize and Su, Yusheng and Yu, Jianxin and Xue, Wei and Zhang, Shanghang and Fu, Jie and Liu, Zhiyuan},
  journal   = {arXiv preprint arXiv:2308.07201},
  year      = {2023}
}

@article{liang2023encouraging,
  title     = {Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate},
  author    = {Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Tu, Zhaopeng and Shi, Shuming},
  journal   = {arXiv preprint arXiv:2305.19118},
  year      = {2023}
}

@article{qwen3,
  title     = {Qwen3 Technical Report},
  author    = {{Qwen Team}},
  journal   = {arXiv preprint arXiv:2505.09388},
  year      = {2025}
}

@misc{lending_data,
  title     = {Lending Club Loan Data},
  howpublished = {\url{https://www.kaggle.com/datasets/wordsforthewise/lending-club}},
  year      = {2018}
}
