{
 "cells": [
  {
   "cell_type": "code",
   "id": "Xnhmb32KVGuTaeGT3D9gArRM",
   "metadata": {
    "tags": [],
    "id": "Xnhmb32KVGuTaeGT3D9gArRM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1769609276265,
     "user_tz": 300,
     "elapsed": 12089,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "source": "import os, re, datetime\nimport pandas as pd\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport openai\n\nMODEL  = \"gpt-5-nano\"\nMETHOD = \"auditor\"  # \"base\" or \"auditor\"\nN_SAMPLES = 5",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df = pd.read_csv(\"../data/AIME_Dataset_1983_2024.csv\")  # 933 rows\ndf.head()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rDmTO2sUxk3U",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1769609277133,
     "user_tz": 300,
     "elapsed": 874,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "e3580097-9dc0-46e4-b281-c169136f04df"
   },
   "id": "rDmTO2sUxk3U",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def create_prompt_base(row):\n    return f\"You are helping Person 1 solve the following math problem: {row['Question']}.\"",
   "metadata": {
    "id": "Xe13kVzQx1zr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1769609277133,
     "user_tz": 300,
     "elapsed": 6,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "Xe13kVzQx1zr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def get_llm_base(prompt):\n    instructions = (\n        \"\\n\\nRespond with two numbers ONLY on two lines. \"\n        \"First, your integer solution. \"\n        \"Second, 0 to implement your solution or 1 to delegate to Person 1.\"\n    )\n    response = openai.chat.completions.create(\n        model=MODEL,\n        messages=[{\"role\": \"user\", \"content\": prompt + instructions}]\n    )\n    lines = [l.strip() for l in response.choices[0].message.content.strip().split('\\n') if l.strip()]\n    try:\n        return {\"pred\": int(lines[0]), \"del\": int(lines[1])}\n    except (ValueError, IndexError):\n        print(f\"Parse error: {response.choices[0].message.content}\")\n        return {\"pred\": None, \"del\": None}",
   "metadata": {
    "id": "8DGlZZ_zyC2k",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1769609277133,
     "user_tz": 300,
     "elapsed": 5,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "8DGlZZ_zyC2k",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def llm(prompt):\n    r = openai.chat.completions.create(model=MODEL, messages=[{\"role\": \"user\", \"content\": prompt}])\n    return r.choices[0].message.content.strip()\n\ndef get_sequential_inference(scenario):\n    try:\n        # Step 1: solve with reasoning\n        thought = llm(f\"{scenario}\\n\\nSolve the math problem. Explain your reasoning in 1 sentence, \"\n                      \"then conclude with 'SOLUTION: <integer>'.\")\n        preds = re.findall(r'SOLUTION:\\s*(\\d+)', thought)\n        pred = preds[-1] if preds else \"0\"\n\n        # Step 2: critique the reasoning\n        critique = llm(f\"PROBLEM: {scenario}\\n\\nPROPOSED SOLUTION: {thought}\\n\\nCritique this reasoning in 1 sentence.\")\n\n        # Step 3: decide whether to implement or delegate\n        decision = llm(f\"PROBLEM: {scenario}\\nSOLUTION AND REASONING: {thought}\\nCRITIQUE: {critique}\\n\\n\"\n                       \"Implement your solution (0) or delegate to Person 1 (1). Respond with the digit ONLY.\")\n\n        return {\"full_thought\": thought, \"pred\": pred, \"critique\": critique, \"del\": decision}\n    except Exception as e:\n        return {\"full_thought\": str(e), \"pred\": \"Err\", \"critique\": \"Err\", \"del\": \"1\"}",
   "metadata": {
    "id": "r_pfY_XLyV4A",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1769609277133,
     "user_tz": 300,
     "elapsed": 5,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "r_pfY_XLyV4A",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def call_llm(idx, row):\n    base = create_prompt_base(row)\n    if METHOD == \"base\":\n        result = get_llm_base(base)\n        return {**row, \"prompt\": base, \"llm_prediction\": result[\"pred\"],\n                \"llm_delegate\": result[\"del\"], \"solution\": row[\"Answer\"], \"method\": METHOD}\n    elif METHOD == \"auditor\":\n        result = get_sequential_inference(base)\n        return {**row, \"prompt\": base, \"llm_full_thought\": result[\"full_thought\"],\n                \"llm_prediction\": result[\"pred\"], \"llm_critique\": result[\"critique\"],\n                \"llm_delegate\": result[\"del\"], \"solution\": row[\"Answer\"], \"method\": METHOD}",
   "metadata": {
    "id": "8fQFhdw7z7of",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1769610406915,
     "user_tz": 300,
     "elapsed": 4,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "8fQFhdw7z7of",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "sampled_rows = df.sample(n=N_SAMPLES)\nresults = []\ncompleted = 0\n\ndef call_llm_tracked(idx, row):\n    global completed\n    result = call_llm(idx, row)\n    completed += 1\n    print(f\"[{completed}/{N_SAMPLES}] Done: row {idx}\")\n    return result\n\nprint(f\"Starting {N_SAMPLES} samples | model: {MODEL} | method: {METHOD}\")\nwith ThreadPoolExecutor(max_workers=5) as executor:\n    futures = [executor.submit(call_llm_tracked, idx, row) for idx, row in sampled_rows.iterrows()]\n    for f in as_completed(futures):\n        results.append(f.result())\n\ndf_results = pd.DataFrame(results)\nprint(\"Done.\")\ndf_results",
   "metadata": {
    "id": "nXnv0gnCz_Dg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1769612315255,
     "user_tz": 300,
     "elapsed": 1905635,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "nXnv0gnCz_Dg",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df_results['timestamp'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\nlocal_dir = '../results/AIME'\nos.makedirs(local_dir, exist_ok=True)\nlocal_path = os.path.join(local_dir, f'{METHOD}_{MODEL}.csv')\n\ntry:\n    df_results = pd.concat([pd.read_csv(local_path), df_results], ignore_index=True)\nexcept FileNotFoundError:\n    pass\n\ndf_results.to_csv(local_path, index=False)\nprint(f\"Saved to {local_path}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wrV8aBjA3nwn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1769612315257,
     "user_tz": 300,
     "elapsed": 13,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "a4abea2d-63e7-42ac-f3a7-d90ec351fc86"
   },
   "id": "wrV8aBjA3nwn",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "colab": {
   "provenance": [],
   "name": "AIME"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}