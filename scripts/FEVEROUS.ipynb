{
 "cells": [
  {
   "cell_type": "code",
   "id": "dAT5FgoJktoXbBm6xUJBhkMn",
   "metadata": {
    "tags": [],
    "id": "dAT5FgoJktoXbBm6xUJBhkMn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770222083326,
     "user_tz": 300,
     "elapsed": 12300,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "source": [
    "import sys\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "import matplotlib.pyplot as plt\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# from anthropic import Anthropic\n",
    "\n",
    "# key from delegation OpenAI project\n",
    "\n",
    "\n",
    "# MODEL = \"gpt-4o-2024-05-13\"\n",
    "# MODEL = \"gpt-4o-mini-2024-07-18\"\n",
    "MODEL = \"gpt-5-nano\" # cheaper than 4o-mini!\n",
    "# MODEL = \"o1-2024-12-17\"\n",
    "# MODEL = \"o3-mini-2025-01-31\"\n",
    "# MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "# MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "# list of claude models here: https://docs.anthropic.com/en/docs/about-claude/models/overview\n",
    "# MODEL = \"claude-3-5-haiku@20241022\"\n",
    "# MODEL = \"claude-sonnet-4@20250514\"\n",
    "# MODEL = \"claude-opus-4-20250514\"\n",
    "\n",
    "# list of llama models here: https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-scout?utm_source=chatgpt.com\n",
    "# MODEL = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "# MODEL = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "\n",
    "# initialize client\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    # project=\"accuracy-obsession\",  # this links to my CC! Although credits might be in there before. Exceptions project is exceptions-467800\n",
    "    # project = \"exceptions-467800\",\n",
    "    location=\"us-central1\"\n",
    ")\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load and clean data\n",
    "df = pd.read_json(path_or_buf=\"gs://exceptions-data/LLM Delegation/FEVEROUS/data/feverous_train_challenges.jsonl\", lines=True)\n",
    "df = df.replace('', pd.NA).dropna(how='all')\n",
    "df = df[df[\"label\"] != \"NOT ENOUGH INFO\"]\n",
    "df[\"supports\"] = df[\"label\"].map({\"SUPPORTS\": 1, \"REFUTES\": 0})\n",
    "df.head()"
   ],
   "metadata": {
    "id": "q0QAJXo2Pgmi",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770222090898,
     "user_tz": 300,
     "elapsed": 7584,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "2be36c00-3fb3-40ec-b83d-3e4001be4901"
   },
   "id": "q0QAJXo2Pgmi",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id                                              claim     label  \\\n",
       "1  24435  Michael Folivi competed with ten teams from 20...   REFUTES   \n",
       "2  14802  Asiatic Society of Bangladesh(housed in Nimtal...  SUPPORTS   \n",
       "3  28540  Lindfield railway station has 3 bus routes, in...  SUPPORTS   \n",
       "4  71874  Mukaradeeb('Wolf's Den') is a city in Iraq nea...  SUPPORTS   \n",
       "5  70296  Herbivore men was coined by Maki Fukasawa and ...  SUPPORTS   \n",
       "\n",
       "                                            evidence  \\\n",
       "1  [{'content': ['Michael Folivi_cell_1_2_0', 'Mi...   \n",
       "2  [{'content': ['Asiatic Society of Bangladesh_s...   \n",
       "3  [{'content': ['Lindfield railway station_sente...   \n",
       "4  [{'content': ['Mukaradeeb_sentence_1', 'Mukara...   \n",
       "5  [{'content': ['Herbivore men_sentence_1', 'Her...   \n",
       "\n",
       "                                annotator_operations  \\\n",
       "1  [{'operation': 'start', 'value': 'start', 'tim...   \n",
       "2  [{'operation': 'start', 'value': 'start', 'tim...   \n",
       "3  [{'operation': 'start', 'value': 'start', 'tim...   \n",
       "4  [{'operation': 'start', 'value': 'start', 'tim...   \n",
       "5  [{'operation': 'start', 'value': 'start', 'tim...   \n",
       "\n",
       "                   challenge  supports  \n",
       "1        Numerical Reasoning         0  \n",
       "2                      Other         1  \n",
       "3                      Other         1  \n",
       "4  Combining Tables and Text         1  \n",
       "5        Multi-hop Reasoning         1  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-f8520b0d-908b-44ba-94cf-43a2c62331be\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>annotator_operations</th>\n",
       "      <th>challenge</th>\n",
       "      <th>supports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24435</td>\n",
       "      <td>Michael Folivi competed with ten teams from 20...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[{'content': ['Michael Folivi_cell_1_2_0', 'Mi...</td>\n",
       "      <td>[{'operation': 'start', 'value': 'start', 'tim...</td>\n",
       "      <td>Numerical Reasoning</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14802</td>\n",
       "      <td>Asiatic Society of Bangladesh(housed in Nimtal...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[{'content': ['Asiatic Society of Bangladesh_s...</td>\n",
       "      <td>[{'operation': 'start', 'value': 'start', 'tim...</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28540</td>\n",
       "      <td>Lindfield railway station has 3 bus routes, in...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[{'content': ['Lindfield railway station_sente...</td>\n",
       "      <td>[{'operation': 'start', 'value': 'start', 'tim...</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71874</td>\n",
       "      <td>Mukaradeeb('Wolf's Den') is a city in Iraq nea...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[{'content': ['Mukaradeeb_sentence_1', 'Mukara...</td>\n",
       "      <td>[{'operation': 'start', 'value': 'start', 'tim...</td>\n",
       "      <td>Combining Tables and Text</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70296</td>\n",
       "      <td>Herbivore men was coined by Maki Fukasawa and ...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[{'content': ['Herbivore men_sentence_1', 'Her...</td>\n",
       "      <td>[{'operation': 'start', 'value': 'start', 'tim...</td>\n",
       "      <td>Multi-hop Reasoning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8520b0d-908b-44ba-94cf-43a2c62331be')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-f8520b0d-908b-44ba-94cf-43a2c62331be button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-f8520b0d-908b-44ba-94cf-43a2c62331be');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df",
       "repr_error": "0"
      }
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# scenario builder\n",
    "def create_prompt_base(row):\n",
    "    return f\"You are helping Person 1 with fact-checking. This statement needs to be checked: {row['claim']}.\"\n"
   ],
   "metadata": {
    "id": "_UjDeXn1Se_u",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770222090898,
     "user_tz": 300,
     "elapsed": 8,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "_UjDeXn1Se_u",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "create_prompt_base(df.iloc[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "3QKE7sGzTBjv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770222090898,
     "user_tz": 300,
     "elapsed": 8,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "9e0aebdd-e8af-4c36-d2a5-8b9be1857e7a"
   },
   "id": "3QKE7sGzTBjv",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'You are helping Person 1 with fact-checking. This statement needs to be checked: Michael Folivi competed with ten teams from 2016 to 2021, appearing in 54 games and making seven goals in total..'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google import genai\n",
    "\n",
    "\n",
    "# base LLM prompt\n",
    "def get_llm_base(base_prompt, model_id=MODEL):\n",
    "\n",
    "    # 1. Construct the one-shot prompt\n",
    "    instructions = (\n",
    "        \"\\n\\nRespond with two numbers ONLY on two lines. \"\n",
    "        \"First, respond 1 (support) or 0 (refute) based on your prediction of Person 1's true decision. \\n\"\n",
    "        \"Second, respond 0 to implement your prediction of Person 1's true decision, \"\n",
    "        \"or 1 if you would like to delegate to Person 1 so they can implement their true decision. \\n\"\n",
    "        \"A decision is considered 'correct' if it is equal to Person 1's true decision.\"\n",
    "    )\n",
    "\n",
    "    full_prompt = base_prompt + instructions\n",
    "\n",
    "    # 2. Call the LLM\n",
    "\n",
    "    # openAI version\n",
    "    messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "    response = openai.chat.completions.create(model=model_id, messages=messages)\n",
    "    lines = response.choices[0].message.content.strip().split('\\n')\n",
    "\n",
    "\n",
    "    # 3. Parse the response (extract numbers from the two lines)\n",
    "    try:\n",
    "\n",
    "\n",
    "        # Filter out empty lines if the LLM added extra spacing\n",
    "        clean_lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "        # save integer if there is one\n",
    "        try:\n",
    "            prediction = int(clean_lines[0])\n",
    "            delegation = int(clean_lines[1])\n",
    "        except (ValueError, TypeError, IndexError):\n",
    "            prediction = None\n",
    "            delegation = None\n",
    "\n",
    "        return {\n",
    "              \"pred\": prediction,\n",
    "              \"del\": delegation\n",
    "          }\n",
    "\n",
    "    except (ValueError, IndexError):\n",
    "        print(f\"Error parsing LLM response: {response.text}\")\n",
    "        return None, None"
   ],
   "metadata": {
    "id": "cLV3dEHuTFlQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770222090899,
     "user_tz": 300,
     "elapsed": 7,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "cLV3dEHuTFlQ",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# auditor (multiple LLMs: a separate LLM does the critiquing and another one does the deciding)\n",
    "def get_sequential_inference(base_scenario, model_name):\n",
    "    try:\n",
    "        # --- STEP 1: PREDICT & EXPLAIN (The Generator) ---\n",
    "        # We ask for the \"thought process\" first. This improves the prediction.\n",
    "        p1 = (f\"{base_scenario}\\n\\n\"\n",
    "              \"TASK: Predict Person 1's decision. First, explain your reasoning in 1 sentence. \"\n",
    "              \"Then, conclude with 'PREDICTION: 1' (support) or 'PREDICTION: 0' (refute).\")\n",
    "\n",
    "        messages_1 = [{\"role\": \"user\", \"content\": p1}]\n",
    "\n",
    "        # OpenAI version\n",
    "        r1 = openai.chat.completions.create(model=model_name, messages=messages_1)\n",
    "        full_output = r1.choices[0].message.content.strip()\n",
    "\n",
    "        # gemini version\n",
    "        # r1 = client.models.generate_content(\n",
    "        #    model=MODEL,\n",
    "        #    contents=p1\n",
    "        # )\n",
    "\n",
    "        # 2. Extract the output (This replaces r1.choices[0].message.content)\n",
    "        # full_output = r1.text.strip()\n",
    "\n",
    "\n",
    "        import re\n",
    "        preds = re.findall(r'PREDICTION:\\s*([01])', full_output)\n",
    "        pred = preds[-1] if preds else \"0\"\n",
    "\n",
    "        # --- STEP 2: CRITIQUE (The Auditor) ---\n",
    "        # NEW CHAT: A fresh perspective to find flaws in the logic above.\n",
    "        p2 = (f\"SCENARIO: {base_scenario}\\n\\n\"\n",
    "              f\"PROPOSED LOGIC: {full_output}\\n\\n\"\n",
    "              \"TASK: Critique this reasoning (1 sentence).\")\n",
    "\n",
    "        messages_2 = [{\"role\": \"user\", \"content\": p2}]\n",
    "\n",
    "        # OpenAI version\n",
    "        r2 = openai.chat.completions.create(model=model_name, messages=messages_2)\n",
    "        critique = r2.choices[0].message.content.strip()\n",
    "\n",
    "        # gemini version\n",
    "        # r2 = client.models.generate_content(\n",
    "        #    model=MODEL,\n",
    "        #    contents=p2\n",
    "        # )\n",
    "        # critique = r2.text.strip()\n",
    "\n",
    "\n",
    "        # --- STEP 3: FINAL DECISION (The Judge) ---\n",
    "        # NEW CHAT: The final decision-maker weigh the prediction vs. the critique.\n",
    "        p3 = (f\"SCENARIO: {base_scenario}\\n\"\n",
    "              f\"PREDICTION & REASONING: {full_output}\\n\"\n",
    "              f\"CRITIQUE: {critique}\\n\\n\"\n",
    "              \"TASK: Implement the prediction (0) or delegate to Person 1 (1). The ground truth is Person 1's preference.\\n\"\n",
    "              \"Respond with the digit ONLY.\")\n",
    "\n",
    "        messages_3 = [{\"role\": \"user\", \"content\": p3}]\n",
    "\n",
    "        # OpenAI version\n",
    "        r3 = openai.chat.completions.create(model=model_name, messages=messages_3)\n",
    "        delegate = r3.choices[0].message.content.strip()\n",
    "\n",
    "        # gemini version\n",
    "        # r3 = client.models.generate_content(\n",
    "        #    model=MODEL,\n",
    "        #    contents=p3\n",
    "        # )\n",
    "        # delegate = r3.text.strip()\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"full_thought\": full_output,\n",
    "            \"pred\": pred,\n",
    "            \"critique\": critique,\n",
    "            \"del\": delegate\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"full_thought\": str(e), \"pred\": \"Err\", \"critique\": \"Err\", \"del\": \"1\"}"
   ],
   "metadata": {
    "id": "l-wRlUtRTebH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770222090899,
     "user_tz": 300,
     "elapsed": 7,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "l-wRlUtRTebH",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_sequential_inference(create_prompt_base(df.iloc[0]), MODEL)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33r-dV_ZT8sX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770222116939,
     "user_tz": 300,
     "elapsed": 26046,
     "user": {
      "displayName": "",
      "userId": ""
     }
    },
    "outputId": "24803871-010b-46cc-beae-034288918c30"
   },
   "id": "33r-dV_ZT8sX",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'full_thought': 'Reasoning: Given Folivi\u2019s career history up to 2021, having ten different teams with 54 appearances and seven goals across five years seems implausible and unlikely. \\nPREDICTION: 0',\n",
       " 'pred': '0',\n",
       " 'critique': 'That reasoning relies on subjective plausibility without citing sources and ignores what \"ten teams\" could include (loans, youth/non-league stints), so it cannot reliably judge the statement without checking actual records.',\n",
       " 'del': '1'}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "\n",
    "# THE MULTI-THREADED RUNNER\n",
    "def call_llm(idx, row):\n",
    "\n",
    "    ########################\n",
    "    # DECIDE method to run #\n",
    "    ########################\n",
    "    # method to run\n",
    "    TEMP_METHOD = \"base\"\n",
    "    # TEMP_METHOD = \"sft\"\n",
    "    # TEMP_METHOD = \"auditor\"\n",
    "\n",
    "    # base\n",
    "    if TEMP_METHOD == \"base\":\n",
    "\n",
    "      base = create_prompt_base(row)\n",
    "\n",
    "      # Run the base LLM\n",
    "      result = get_llm_base(base, MODEL) # standard call\n",
    "\n",
    "      # 3. Save everything back to a copy of the row\n",
    "      row_copy = row.copy()\n",
    "      row_copy['prompt'] = base # the prompt\n",
    "      row_copy['llm_prediction'] = result['pred']\n",
    "      row_copy['llm_delegate'] = result['del']\n",
    "      row_copy['human_response'] = row['supports']  # Ground truth for accuracy\n",
    "      row_copy['method'] = TEMP_METHOD # avoid mixing up methods\n",
    "\n",
    "      return row_copy\n",
    "\n",
    "\n",
    "\n",
    "    # auditor\n",
    "    if TEMP_METHOD == \"auditor\":\n",
    "\n",
    "      base = create_prompt_base(row)\n",
    "\n",
    "      # Run the 3-step Metacognitive Loop\n",
    "      # This now returns: pred, full_thought, critique, del\n",
    "      # result = get_sequential_inference(base, MODEL) # different llms act as auditors\n",
    "      result = get_sequential_inference(base, MODEL) # LLM is its own auditor\n",
    "\n",
    "      # Save everything back to a copy of the row\n",
    "      row_copy = row.copy()\n",
    "      row_copy['prompt'] = base # the prompt\n",
    "      row_copy['llm_full_thought'] = result['full_thought'] # The Reasoning + Prediction\n",
    "      row_copy['llm_prediction'] = result['pred']         # Extracted digit from Step 1\n",
    "      row_copy['llm_critique'] = result['critique']       # The Auditor's critique\n",
    "      row_copy['llm_delegate'] = result['del']           # The Judge's final decision\n",
    "      row_copy['human_response'] = row['supports']         # Ground truth for accuracy\n",
    "      row_copy['method'] = TEMP_METHOD # avoid mixing up methods\n",
    "\n",
    "      return row_copy\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "nsfsrR2NUd2f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770223541260,
     "user_tz": 300,
     "elapsed": 218,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "nsfsrR2NUd2f",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get sample df (that wasn't used to train the random forest)\n",
    "N_SAMPLES = 200\n",
    "sampled_rows = df.sample(n=N_SAMPLES) # can't have a random state, or we'll do the same values over and over!\n",
    "\n",
    "# initialize results\n",
    "results = []\n",
    "\n",
    "# make call\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(call_llm, idx, row) for idx, row in sampled_rows.iterrows()]\n",
    "    for f in as_completed(futures):\n",
    "        results.append(f.result())\n",
    "\n",
    "# save\n",
    "df_results = pd.DataFrame(results)\n"
   ],
   "metadata": {
    "id": "lvWbcBDCU3qV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770224102601,
     "user_tz": 300,
     "elapsed": 559378,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "lvWbcBDCU3qV",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "# write file; add timestamp\n",
    "df_results['timestamp'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "###########################################\n",
    "# MAKE SURE THE FILE MATCHES TEMP_METHOD! #\n",
    "###########################################\n",
    "path = 'gs://exceptions-data/LLM Delegation/FEVEROUS/Results/base_' + MODEL + '.csv'\n",
    "# path = 'gs://exceptions-data/LLM Delegation/FEVEROUS/Results/auditor_' + MODEL + '.csv'\n",
    "\n",
    "# Load, append, and re-save\n",
    "try:\n",
    "    df_existing = pd.read_csv(path)\n",
    "    df_results = pd.concat([df_existing, df_results], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "df_results.to_csv(path, index=False)"
   ],
   "metadata": {
    "id": "6dxjI4JbU9oH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770224103629,
     "user_tz": 300,
     "elapsed": 327,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "6dxjI4JbU9oH",
   "execution_count": 13,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "colab": {
   "provenance": [],
   "name": "FEVEROUS"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}