{
 "cells": [
  {
   "cell_type": "code",
   "id": "dqUAizUV2qSEe74O4Cjff0gs",
   "metadata": {
    "tags": [],
    "id": "dqUAizUV2qSEe74O4Cjff0gs"
   },
   "source": [
    "# !pip install anthropic"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "import matplotlib.pyplot as plt\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# from anthropic import Anthropic\n",
    "\n",
    "# key from delegation OpenAI project\n",
    "\n",
    "\n",
    "# MODEL = \"gpt-4o-2024-05-13\"\n",
    "# MODEL = \"gpt-4o-mini-2024-07-18\"\n",
    "MODEL = \"gpt-5-nano\" # cheaper than 4o-mini!\n",
    "# MODEL = \"o1-2024-12-17\"\n",
    "# MODEL = \"o3-mini-2025-01-31\"\n",
    "# MODEL = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "# MODEL = \"gemini-2.5-flash\"\n",
    "\n",
    "# list of claude models here: https://docs.anthropic.com/en/docs/about-claude/models/overview\n",
    "# MODEL = \"claude-3-5-haiku@20241022\"\n",
    "# MODEL = \"claude-sonnet-4@20250514\"\n",
    "# MODEL = \"claude-opus-4-20250514\"\n",
    "\n",
    "# list of llama models here: https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/llama/llama4-scout?utm_source=chatgpt.com\n",
    "# MODEL = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "# MODEL = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "\n",
    "# initialize client\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    # project=\"accuracy-obsession\",  # this links to my CC! Although credits might be in there before. Exceptions project is exceptions-467800\n",
    "    project = \"exceptions-467800\",\n",
    "    location=\"us-central1\"\n",
    ")\n"
   ],
   "metadata": {
    "id": "R7NEzjfs0_pf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770228411834,
     "user_tz": 300,
     "elapsed": 9670,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "R7NEzjfs0_pf",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load all Movielens data (large!)\n",
    "# df_movies = pd.read_csv(\"gs://exceptions-data/LLM Delegation/MovieLens/data/movies.csv\")\n",
    "# df_ratings = pd.read_csv(\"gs://exceptions-data/LLM Delegation/MovieLens/data/ratings.csv\")\n",
    "\n",
    "# merge and save data (32M rows!)\n",
    "# df_all = df_ratings.merge(df_movies, left_on = 'movieId', right_on = 'movieId')\n",
    "# df_all.to_csv(\"gs://exceptions-data/LLM Delegation/MovieLens/data/movies_and_ratings.csv\")\n",
    "\n",
    "# save subset (for speed)\n",
    "# temp = df_all.tail(1000000)\n",
    "# temp.to_csv(\"gs://exceptions-data/LLM Delegation/MovieLens/data/movies_and_ratings_last1000000.csv\")"
   ],
   "metadata": {
    "id": "KEeOYr9B1ENS"
   },
   "id": "KEeOYr9B1ENS",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load subset of merged data\n",
    "df = pd.read_csv(\"gs://exceptions-data/LLM Delegation/MovieLens/data/movies_and_ratings_last1000000.csv\")"
   ],
   "metadata": {
    "id": "hwZ_84gbdwrl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770228416818,
     "user_tz": 300,
     "elapsed": 4985,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "hwZ_84gbdwrl",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "##################\n",
    "#### Train OLS ###\n",
    "##################\n",
    "\n",
    "# clean training dataset\n",
    "df_ols = df.copy()\n",
    "\n",
    "# extract 4-digit year at end of title, else 0\n",
    "df_ols['year'] = (\n",
    "    df_ols['title']\n",
    "    .str.extract(r'\\((\\d{4})\\)$', expand=False)\n",
    "    .pipe(pd.to_numeric, errors='coerce')\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# fast vectorized genre one-hot encoding (spreads)\n",
    "genre_counts = (\n",
    "    df_ols['genres']\n",
    "    .fillna('')\n",
    "    .str.get_dummies(sep='|')\n",
    "    .astype('int8')\n",
    ")\n",
    "\n",
    "# join back to dataframe\n",
    "df_ols = df_ols.join(genre_counts)\n",
    "\n",
    "\n",
    "# build formula: C(...) tells statsmodels to treat as categorical\n",
    "# but if I do categorical, everything breaks!\n",
    "# no userId or movieId because then everything breaks\n",
    "formula = (\n",
    "    \"rating ~ year + \"\n",
    "    \"Action + Adventure + Animation + Children + Comedy + Crime + \"\n",
    "    \"Documentary + Drama + Fantasy + Q('Film-Noir') + Horror + IMAX + \"\n",
    "    \"Musical + Mystery + Romance + Q('Sci-Fi') + Thriller + War + Western\"\n",
    ")\n",
    "\n",
    "# create merge key (string is safest)\n",
    "df_ols['user_movie_key'] = (\n",
    "    df_ols['userId'].astype(str) + \"_\" + df_ols['movieId'].astype(str)\n",
    ")\n",
    "\n",
    "# train / test split (keep indices for later!)\n",
    "train_df, test_df = train_test_split(\n",
    "    df_ols, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# mark train / test\n",
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()\n",
    "train_df['split'] = 'train'\n",
    "test_df['split'] = 'test'\n",
    "\n",
    "# fit model on training data\n",
    "ols = smf.ols(formula=formula, data=train_df).fit()\n",
    "\n",
    "# optional: full model summary\n",
    "print(ols.summary())\n",
    "\n",
    "# generate predictions for ALL rows\n",
    "df_ols['pred'] = ols.predict(df_ols)\n",
    "\n",
    "# keep only what we need for merging back\n",
    "pred_df = df_ols[['user_movie_key', 'pred']]\n",
    "\n",
    "split_df = pd.concat([\n",
    "    train_df[['user_movie_key', 'split']],\n",
    "    test_df[['user_movie_key', 'split']]\n",
    "])\n",
    "\n",
    "# save r^2\n",
    "r2 = ols.rsquared\n",
    "\n",
    "# merge predictions + split info back to original df\n",
    "df['user_movie_key'] = (\n",
    "    df['userId'].astype(str) + \"_\" + df['movieId'].astype(str)\n",
    ")\n",
    "df = df.merge(pred_df, on='user_movie_key', how='left')\n",
    "df = df.merge(split_df, on='user_movie_key', how='left')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p00s2flYlDRy",
    "outputId": "1de1408c-0fcb-458c-a478-3227e14a39a6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770228443929,
     "user_tz": 300,
     "elapsed": 27112,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "p00s2flYlDRy",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 rating   R-squared:                       0.033\n",
      "Model:                            OLS   Adj. R-squared:                  0.033\n",
      "Method:                 Least Squares   F-statistic:                     1374.\n",
      "Date:                Wed, 04 Feb 2026   Prob (F-statistic):               0.00\n",
      "Time:                        18:07:19   Log-Likelihood:            -1.1614e+06\n",
      "No. Observations:              800000   AIC:                         2.323e+06\n",
      "Df Residuals:                  799979   BIC:                         2.323e+06\n",
      "Df Model:                          20                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "Intercept          3.8330      0.032    121.213      0.000       3.771       3.895\n",
      "year              -0.0002   1.58e-05    -11.799      0.000      -0.000      -0.000\n",
      "Action            -0.1138      0.003    -36.218      0.000      -0.120      -0.108\n",
      "Adventure          0.0758      0.003     22.578      0.000       0.069       0.082\n",
      "Animation          0.3161      0.006     50.580      0.000       0.304       0.328\n",
      "Children          -0.2553      0.006    -43.537      0.000      -0.267      -0.244\n",
      "Comedy            -0.1137      0.003    -38.501      0.000      -0.120      -0.108\n",
      "Crime              0.2019      0.003     58.069      0.000       0.195       0.209\n",
      "Documentary        0.1330      0.011     12.579      0.000       0.112       0.154\n",
      "Drama              0.1942      0.003     67.618      0.000       0.189       0.200\n",
      "Fantasy            0.0370      0.004      9.212      0.000       0.029       0.045\n",
      "Q('Film-Noir')     0.1963      0.012     16.378      0.000       0.173       0.220\n",
      "Horror            -0.1943      0.005    -42.319      0.000      -0.203      -0.185\n",
      "IMAX               0.0403      0.006      7.078      0.000       0.029       0.051\n",
      "Musical            0.0580      0.007      8.589      0.000       0.045       0.071\n",
      "Mystery            0.1186      0.005     26.228      0.000       0.110       0.127\n",
      "Romance            0.0076      0.003      2.304      0.021       0.001       0.014\n",
      "Q('Sci-Fi')        0.0476      0.003     13.654      0.000       0.041       0.054\n",
      "Thriller          -0.0534      0.003    -17.135      0.000      -0.059      -0.047\n",
      "War                0.2220      0.006     40.165      0.000       0.211       0.233\n",
      "Western            0.0822      0.009      9.507      0.000       0.065       0.099\n",
      "==============================================================================\n",
      "Omnibus:                    54729.627   Durbin-Watson:                   1.998\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            66628.215\n",
      "Skew:                          -0.689   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.319   Cond. No.                     5.46e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.46e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def create_prompt_base(row):\n",
    "\n",
    "    # 1. check if given row has at least 7 ratings\n",
    "    temp_ID = df.iloc[row]['userId']\n",
    "    temp_ratings = df.loc[df['userId'] == temp_ID].shape[0]\n",
    "\n",
    "    if temp_ratings < 7:\n",
    "        return \"Not enough reviews per user to generate this prompt.\", None, None\n",
    "\n",
    "    # grab user data\n",
    "    user_data = df[df['userId'] == temp_ID].copy()\n",
    "\n",
    "    # 2. Select two movies with DIFFERENT ratings\n",
    "    shuffled_user_data = user_data.sample(frac=1, random_state=None)\n",
    "\n",
    "    test_movie_1 = shuffled_user_data.iloc[0]\n",
    "    test_movie_2 = None\n",
    "\n",
    "    for i in range(1, len(shuffled_user_data)):\n",
    "        if shuffled_user_data.iloc[i]['rating'] != test_movie_1['rating']:\n",
    "            test_movie_2 = shuffled_user_data.iloc[i]\n",
    "            used_indices = [0, i]\n",
    "            break\n",
    "\n",
    "    if test_movie_2 is None:\n",
    "        return \"Could not find two movies with different ratings for this user.\", None, None\n",
    "\n",
    "    # 3. Grab 5 other movies for the history\n",
    "    history_pool = shuffled_user_data.drop(shuffled_user_data.index[used_indices])\n",
    "    history_movies = history_pool.head(5)\n",
    "\n",
    "    # 4. Format the Prompt\n",
    "    prompt = \"Person 1 has reviewed the following movies:\\n\\n\"\n",
    "    for _, r in history_movies.iterrows():\n",
    "        prompt += f\"- {r['title']} ({r['genres']}): Rated {r['rating']}/5\\n\"\n",
    "\n",
    "    prompt += \"\\nConsider these two movies they have not seen:\\n\\n\"\n",
    "\n",
    "    test_pair = [test_movie_1, test_movie_2]\n",
    "    random.shuffle(test_pair)\n",
    "\n",
    "    for movie in test_pair:\n",
    "        prompt += f\"- {movie['title']} ({movie['genres']})\\n\"\n",
    "\n",
    "    # 5. Return prompt, true ratings, and predicted ratings\n",
    "    answer_key = {\n",
    "        test_movie_1['title']: test_movie_1['rating'],\n",
    "        test_movie_2['title']: test_movie_2['rating']\n",
    "    }\n",
    "\n",
    "    pred_key = {\n",
    "        test_movie_1['title']: test_movie_1['pred'],\n",
    "        test_movie_2['title']: test_movie_2['pred']\n",
    "    }\n",
    "\n",
    "    return prompt, answer_key, pred_key\n"
   ],
   "metadata": {
    "id": "MkG3i-L75au5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770228443929,
     "user_tz": 300,
     "elapsed": 23,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "MkG3i-L75au5",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test one prompt\n",
    "create_prompt_base(0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HECNsUxeaSpH",
    "outputId": "b7ec71db-d8e0-4f08-81dd-f736eb96fec9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770228443929,
     "user_tz": 300,
     "elapsed": 21,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "HECNsUxeaSpH",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(\"Person 1 has reviewed the following movies:\\n\\n- She's Gotta Have It (1986) (Comedy|Romance): Rated 3.0/5\\n- Ladyhawke (1985) (Adventure|Fantasy|Romance): Rated 2.0/5\\n- Tao of Steve, The (2000) (Comedy): Rated 3.0/5\\n- X-Men (2000) (Action|Adventure|Sci-Fi): Rated 4.0/5\\n- Animal House (1978) (Comedy): Rated 2.0/5\\n\\nConsider these two movies they have not seen:\\n\\n- Sarafina! (1992) (Drama)\\n- Jagged Edge (1985) (Crime|Romance|Thriller)\\n\",\n",
       " {'Jagged Edge (1985)': 3.0, 'Sarafina! (1992)': 2.0},\n",
       " {'Jagged Edge (1985)': 3.6179953204067603,\n",
       "  'Sarafina! (1992)': 3.654780256109197})"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google import genai\n",
    "\n",
    "\n",
    "# base LLM prompt\n",
    "def get_llm_base(base_prompt, model_id=MODEL):\n",
    "\n",
    "    # 1. Construct the one-shot prompt\n",
    "    instructions = (\n",
    "        \"\\n\\nRespond with two numbers ONLY on two lines. \"\n",
    "        \"First, respond 1 (first movie) or 2 (second movie) based on your prediction of which movie Person 1 would rate higher. \\n\"\n",
    "        \"Second, respond 0 to recommend the movie you predicted, \"\n",
    "        \"or 1 if you would like to delegate to Person 1 so they can choose the movie. Person 1 will always choose the movie they would rate higher \\n\"\n",
    "        \"A decision is considered 'correct' if it is the movie Person 1 would rate higher.\"\n",
    "    )\n",
    "\n",
    "    full_prompt = base_prompt + instructions\n",
    "\n",
    "    # 2. Call the LLM\n",
    "\n",
    "    # openAI version\n",
    "    messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "    response = openai.chat.completions.create(model=model_id, messages=messages)\n",
    "    lines = response.choices[0].message.content.strip().split('\\n')\n",
    "\n",
    "\n",
    "    # gemini version\n",
    "    # response = client.models.generate_content(\n",
    "    #     model=model_id,\n",
    "    #     contents=full_prompt\n",
    "    # )\n",
    "    # lines = response.text.strip().split('\\n')\n",
    "\n",
    "    # 3. Parse the response (extract numbers from the two lines)\n",
    "    try:\n",
    "\n",
    "\n",
    "        # Filter out empty lines if the LLM added extra spacing\n",
    "        clean_lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "        # save integer if there is one\n",
    "        try:\n",
    "            prediction = int(clean_lines[0])\n",
    "            delegation = int(clean_lines[1])\n",
    "        except (ValueError, TypeError, IndexError):\n",
    "            prediction = None\n",
    "            delegation = None\n",
    "\n",
    "        return {\n",
    "              \"pred\": prediction,\n",
    "              \"del\": delegation\n",
    "          }\n",
    "\n",
    "\n",
    "    except (ValueError, IndexError):\n",
    "        print(f\"Error parsing LLM response: {response.text}\")\n",
    "        return None, None"
   ],
   "metadata": {
    "id": "bVAITAHMjHc1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770228443929,
     "user_tz": 300,
     "elapsed": 14,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "bVAITAHMjHc1",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from google import genai\n",
    "\n",
    "# sequential inference (generator -> auditor -> judge)\n",
    "def get_sequential_inference(base_prompt, model_id=MODEL):\n",
    "    try:\n",
    "        # --- STEP 1: PREDICT & EXPLAIN (Generator) ---\n",
    "        p1 = (\n",
    "            f\"{base_prompt}\\n\\n\"\n",
    "            \"TASK: Predict which movie Person 1 would rate higher.\\n\"\n",
    "            \"Explain your reasoning in 1 sentence. Then conclude with exactly:\\n\"\n",
    "            \"'PREDICTION: 1' (first movie) or 'PREDICTION: 2' (second movie).\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # OpenAI version\n",
    "        messages = [{\"role\": \"user\", \"content\": p1}]\n",
    "        r1 = openai.chat.completions.create(model=model_id, messages=messages)\n",
    "        full_output = r1.choices[0].message.content.strip()\n",
    "\n",
    "        # gemini version?\n",
    "        # r1 = client.models.generate_content(model=model_id, contents=p1)\n",
    "        # full_output = r1.text.strip()\n",
    "\n",
    "        preds = re.findall(r'PREDICTION:\\s*([12])', full_output)\n",
    "        pred = int(preds[-1]) if preds else 1  # default to 1 if parsing fails\n",
    "\n",
    "        # --- STEP 2: CRITIQUE (Auditor) ---\n",
    "        p2 = (\n",
    "            f\"PROMPT:\\n{base_prompt}\\n\\n\"\n",
    "            f\"PROPOSED LOGIC:\\n{full_output}\\n\\n\"\n",
    "            \"TASK: Critique this reasoning in 1 sentence.\"\n",
    "        )\n",
    "\n",
    "        # OpenAI version\n",
    "        messages = [{\"role\": \"user\", \"content\": p2}]\n",
    "        r2 = openai.chat.completions.create(model=model_id, messages=messages)\n",
    "        critique = r2.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "        # gemini version?\n",
    "        # r2 = client.models.generate_content(model=model_id, contents=p2)\n",
    "        # critique = r2.text.strip()\n",
    "\n",
    "        # --- STEP 3: FINAL DECISION (Judge) ---\n",
    "        p3 = (\n",
    "            f\"PROMPT:\\n{base_prompt}\\n\\n\"\n",
    "            f\"PREDICTION & REASONING:\\n{full_output}\\n\\n\"\n",
    "            f\"CRITIQUE:\\n{critique}\\n\\n\"\n",
    "            \"TASK: Respond with two numbers ONLY on two lines.\\n\"\n",
    "            \"Line 1: output 1 or 2 for which movie Person 1 would rate higher.\\n\"\n",
    "            \"Line 2: output 0 to implement your prediction, or 1 to delegate to Person 1.\\n\"\n",
    "            \"A decision is correct if it matches the movie Person 1 would rate higher.\"\n",
    "        )\n",
    "\n",
    "        # openai version\n",
    "        messages = [{\"role\": \"user\", \"content\": p3}]\n",
    "        r3 = openai.chat.completions.create(model=model_id, messages=messages)\n",
    "        final_text = r3.choices[0].message.content.strip()\n",
    "\n",
    "        # gemini version\n",
    "        # r3 = client.models.generate_content(model=model_id, contents=p3)\n",
    "        # final_text = r3.text.strip()\n",
    "\n",
    "        # parse final response (two digits on two lines)\n",
    "        lines = [ln.strip() for ln in final_text.split('\\n') if ln.strip()]\n",
    "        final_pred = int(re.findall(r'[12]', lines[0])[0]) if len(lines) > 0 else pred\n",
    "        final_del = int(re.findall(r'[01]', lines[1])[0]) if len(lines) > 1 else 1\n",
    "\n",
    "        return {\n",
    "            \"full_thought\": full_output,\n",
    "            \"pred\": final_pred,\n",
    "            \"critique\": critique,\n",
    "            \"del\": final_del\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in sequential inference: {e}\")\n",
    "        return {\"full_thought\": str(e), \"pred\": None, \"critique\": None, \"del\": None}\n"
   ],
   "metadata": {
    "id": "F754ozRF4CyE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770228443929,
     "user_tz": 300,
     "elapsed": 12,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "F754ozRF4CyE",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_llm_ols(base_prompt, ols_pred1, ols_pred2, r2, model_id=MODEL):\n",
    "\n",
    "\n",
    "    # round for ease of use\n",
    "    ols_pred1 = round(ols_pred1, 2)\n",
    "    ols_pred2 = round(ols_pred2, 2)\n",
    "    r2 = round(r2, 2)\n",
    "\n",
    "    # 1. Construct the one-shot prompt\n",
    "    instructions = (\n",
    "        f\"An OLS model trained on a similar dataset predicts Person 1 would rate the first movie {ols_pred1} and the second movie {ols_pred2}.\"\n",
    "        f\"The OLS model has an R-squared of {r2}. \"\n",
    "        \"\\n\\nRespond with two numbers ONLY on two lines. \"\n",
    "        \"First, respond 1 (first movie) or 2 (second movie) based on your prediction of which movie Person 1 would rate higher. \\n\"\n",
    "        \"Second, respond 0 to recommend the movie you predicted, \"\n",
    "        \"or 1 if you would like to delegate to Person 1 so they can choose the movie. Person 1 will always choose the movie they would rate higher \\n\"\n",
    "        \"A decision is considered 'correct' if it is the movie Person 1 would rate higher.\"\n",
    "    )\n",
    "\n",
    "    full_prompt = base_prompt + instructions\n",
    "\n",
    "    # 2. Call the LLM\n",
    "\n",
    "    # OpenAI version\n",
    "    messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "    response = openai.chat.completions.create(model=model_id, messages=messages)\n",
    "    lines = response.choices[0].message.content.strip().split('\\n')\n",
    "\n",
    "    # gemini version\n",
    "    # response = client.models.generate_content(\n",
    "    #     model=model_id,\n",
    "    #     contents=full_prompt\n",
    "    # )\n",
    "    # lines = response.text.strip().split('\\n')\n",
    "\n",
    "    # 3. Parse the response (extract numbers from the two lines)\n",
    "    try:\n",
    "\n",
    "        # Filter out empty lines if the LLM added extra spacing\n",
    "        clean_lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "        # save integer if there is one\n",
    "        try:\n",
    "            prediction = int(clean_lines[0])\n",
    "            delegation = int(clean_lines[1])\n",
    "        except (ValueError, TypeError, IndexError):\n",
    "            prediction = None\n",
    "            delegation = None\n",
    "\n",
    "        return {\n",
    "              \"pred\": prediction,\n",
    "              \"del\": delegation\n",
    "          }\n",
    "\n",
    "    except (ValueError, IndexError):\n",
    "        print(f\"Error parsing LLM response: {response.text}\")\n",
    "        return None, None\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "1ipKOTqzsScS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770228443929,
     "user_tz": 300,
     "elapsed": 12,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "1ipKOTqzsScS",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "\n",
    "# THE MULTI-THREADED RUNNER\n",
    "def call_llm(row_idx):\n",
    "\n",
    "\n",
    "    # avoid rate limits\n",
    "    # time.sleep(10)\n",
    "\n",
    "    ########################\n",
    "    # DECIDE method to run #\n",
    "    ########################\n",
    "    # method to run\n",
    "    # TEMP_METHOD = \"base\"\n",
    "    # TEMP_METHOD = \"sft\"\n",
    "    # TEMP_METHOD = \"ols\"\n",
    "    TEMP_METHOD = \"auditor\"\n",
    "\n",
    "    # base\n",
    "    if TEMP_METHOD == \"base\":\n",
    "\n",
    "      # get prompt and answers\n",
    "      base, answer_key, pred_key = create_prompt_base(row_idx)\n",
    "\n",
    "      # unpack in a stable order\n",
    "      titles = list(answer_key.keys())\n",
    "      title_1, title_2 = titles[0], titles[1]\n",
    "\n",
    "      # actual ratings\n",
    "      rating_1 = answer_key[title_1]\n",
    "      rating_2 = answer_key[title_2]\n",
    "      human_response = 1 if rating_1 >= rating_2 else 2\n",
    "\n",
    "      # OLS predictions\n",
    "      ols_pred_1 = pred_key[title_1]\n",
    "      ols_pred_2 = pred_key[title_2]\n",
    "\n",
    "      # run LLM (OLS!)\n",
    "      result = get_llm_base(base, model_id=MODEL)\n",
    "\n",
    "      # grab metadata\n",
    "      user_id = df.iloc[row_idx]['userId']\n",
    "      movie_id_1 = df.loc[df['title'] == title_1, 'movieId'].iloc[0]\n",
    "      movie_id_2 = df.loc[df['title'] == title_2, 'movieId'].iloc[0]\n",
    "\n",
    "      # build output row (new object, not tied to df rows)\n",
    "      row_copy = pd.Series({\n",
    "          'userId': user_id,\n",
    "          'movieId1': movie_id_1,\n",
    "          'movieId2': movie_id_2,\n",
    "          'rating_1': rating_1,\n",
    "          'rating_2': rating_2,\n",
    "          'ols_pred_1': ols_pred_1,\n",
    "          'ols_pred_2': ols_pred_2,\n",
    "          'human_response': human_response,\n",
    "          'prompt': base,\n",
    "          'llm_prediction': result['pred'],\n",
    "          'llm_delegate': result['del'],\n",
    "          'method': TEMP_METHOD\n",
    "      })\n",
    "\n",
    "\n",
    "      return row_copy\n",
    "\n",
    "\n",
    "    # ols\n",
    "    if TEMP_METHOD == \"ols\":\n",
    "\n",
    "      # get prompt and answers\n",
    "      base, answer_key, pred_key = create_prompt_base(row_idx)\n",
    "\n",
    "      # unpack in a stable order\n",
    "      titles = list(answer_key.keys())\n",
    "      title_1, title_2 = titles[0], titles[1]\n",
    "\n",
    "      # actual ratings\n",
    "      rating_1 = answer_key[title_1]\n",
    "      rating_2 = answer_key[title_2]\n",
    "      human_response = 1 if rating_1 >= rating_2 else 2\n",
    "\n",
    "      # OLS predictions\n",
    "      ols_pred_1 = pred_key[title_1]\n",
    "      ols_pred_2 = pred_key[title_2]\n",
    "\n",
    "      # run LLM (OLS!)\n",
    "      result = get_llm_ols(base, ols_pred_1, ols_pred_2, r2, model_id=MODEL)\n",
    "\n",
    "      # grab metadata\n",
    "      user_id = df.iloc[row_idx]['userId']\n",
    "      movie_id_1 = df.loc[df['title'] == title_1, 'movieId'].iloc[0]\n",
    "      movie_id_2 = df.loc[df['title'] == title_2, 'movieId'].iloc[0]\n",
    "\n",
    "      # build output row (new object, not tied to df rows)\n",
    "      row_copy = pd.Series({\n",
    "          'userId': user_id,\n",
    "          'movieId1': movie_id_1,\n",
    "          'movieId2': movie_id_2,\n",
    "          'rating_1': rating_1,\n",
    "          'rating_2': rating_2,\n",
    "          'ols_pred_1': ols_pred_1,\n",
    "          'ols_pred_2': ols_pred_2,\n",
    "          'human_response': human_response,\n",
    "          'prompt': base,\n",
    "          'llm_prediction': result['pred'],\n",
    "          'llm_delegate': result['del'],\n",
    "          'method': TEMP_METHOD\n",
    "      })\n",
    "\n",
    "\n",
    "      return row_copy\n",
    "\n",
    "\n",
    "\n",
    "    # auditor\n",
    "    if TEMP_METHOD == \"auditor\":\n",
    "\n",
    "      # get prompt and answers\n",
    "      base, answer_key, pred_key = create_prompt_base(row_idx)\n",
    "\n",
    "      # unpack in a stable order\n",
    "      titles = list(answer_key.keys())\n",
    "      title_1, title_2 = titles[0], titles[1]\n",
    "\n",
    "      # actual ratings\n",
    "      rating_1 = answer_key[title_1]\n",
    "      rating_2 = answer_key[title_2]\n",
    "      human_response = 1 if rating_1 >= rating_2 else 2\n",
    "\n",
    "      # OLS predictions\n",
    "      ols_pred_1 = pred_key[title_1]\n",
    "      ols_pred_2 = pred_key[title_2]\n",
    "\n",
    "      # run LLM (SEQUENTIAL INFERENCE)\n",
    "      result = get_sequential_inference(base, MODEL)\n",
    "\n",
    "      # grab metadata\n",
    "      user_id = df.iloc[row_idx]['userId']\n",
    "      movie_id_1 = df.loc[df['title'] == title_1, 'movieId'].iloc[0]\n",
    "      movie_id_2 = df.loc[df['title'] == title_2, 'movieId'].iloc[0]\n",
    "\n",
    "      # build output row (new object, not tied to df rows)\n",
    "      row_copy = pd.Series({\n",
    "          'userId': user_id,\n",
    "          'movieId1': movie_id_1,\n",
    "          'movieId2': movie_id_2,\n",
    "          'rating_1': rating_1,\n",
    "          'rating_2': rating_2,\n",
    "          'ols_pred_1': ols_pred_1,\n",
    "          'ols_pred_2': ols_pred_2,\n",
    "          'human_response': human_response,\n",
    "          'prompt': base,\n",
    "          'llm_prediction': result['pred'],\n",
    "          'llm_delegate': result['del'],\n",
    "          'full_thought': result.get('full_thought'),\n",
    "          'critique': result.get('critique'),\n",
    "          'method': TEMP_METHOD\n",
    "      })\n",
    "\n",
    "      return row_copy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "7Fmu1m7t2hrB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770229204508,
     "user_tz": 300,
     "elapsed": 4,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "7Fmu1m7t2hrB",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "\n",
    "# get sample indices from test split only\n",
    "N_SAMPLES = 200 # pretty fast with OpenAI\n",
    "sampled_row_idxs = (\n",
    "    df.loc[df['split'] == 'test']\n",
    "    .sample(n=N_SAMPLES) # can't reproduce seed otherwise we'll always do the same ones\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# initialize results\n",
    "results = []\n",
    "\n",
    "# make call (call_llm should accept row_idx only)\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(call_llm, row_idx) for row_idx in sampled_row_idxs]\n",
    "    for f in as_completed(futures):\n",
    "        results.append(f.result())\n",
    "\n",
    "# save\n",
    "df_results = pd.DataFrame(results)\n"
   ],
   "metadata": {
    "id": "bD5WnmTbyU0V",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770230290712,
     "user_tz": 300,
     "elapsed": 1081617,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "bD5WnmTbyU0V",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "\n",
    "# write file; add timestamp\n",
    "df_results['timestamp'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "###########################################\n",
    "# MAKE SURE THE FILE MATCHES TEMP_METHOD! #\n",
    "###########################################\n",
    "# path = 'gs://exceptions-data/LLM Delegation/MovieLens/Results/base_' + MODEL + '.csv'\n",
    "# path = 'gs://exceptions-data/LLM Delegation/MovieLens/Results/ols_' + MODEL + '.csv'\n",
    "path = 'gs://exceptions-data/LLM Delegation/MovieLens/Results/auditor_' + MODEL + '.csv'\n",
    "\n",
    "# Load, append, and re-save\n",
    "try:\n",
    "    df_existing = pd.read_csv(path)\n",
    "    df_results = pd.concat([df_existing, df_results], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "df_results.to_csv(path, index=False)"
   ],
   "metadata": {
    "id": "qo33nqtS6NVD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1770230291108,
     "user_tz": 300,
     "elapsed": 416,
     "user": {
      "displayName": "",
      "userId": ""
     }
    }
   },
   "id": "qo33nqtS6NVD",
   "execution_count": 17,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "colab": {
   "provenance": [],
   "name": "MovieLens"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}